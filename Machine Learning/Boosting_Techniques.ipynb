{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c81f3e7-883c-476b-b8cf-e284b9a741ad",
   "metadata": {},
   "source": [
    "## Boosting Techniques | Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bcea188-ac88-4918-b7ec-410974296073",
   "metadata": {},
   "source": [
    "## Question 1: What is Boosting in Machine Learning? Explain how it improves weak learners"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc73082-ff33-4feb-9829-6d0febc8dd07",
   "metadata": {},
   "source": [
    "-\n",
    "\n",
    "## **Introduction**\n",
    "\n",
    "**Boosting** is an **ensemble learning technique** in Machine Learning that aims to create a **strong predictive model** by combining multiple **weak learners** (usually simple models like shallow decision trees).  \n",
    "The idea is to train these weak learners **sequentially**, where each new model focuses on the **errors made by the previous ones**.  \n",
    "\n",
    "This step-by-step improvement process helps the ensemble achieve **high accuracy** and **better generalization** than any single model alone.\n",
    "\n",
    "---\n",
    "\n",
    "## **Key Idea Behind Boosting**\n",
    "\n",
    "The main concept of boosting is to **convert weak learners into strong learners** by giving more importance to **data points that were misclassified** in previous iterations.  \n",
    "\n",
    "Initially, all data points are given **equal weight**. After each round:\n",
    "- The model identifies the samples it predicted incorrectly.  \n",
    "- The algorithm **increases the weights** of those misclassified samples so that the next learner focuses more on them.  \n",
    "- This process continues, with each new model correcting the mistakes of the earlier ones.  \n",
    "\n",
    "Finally, the predictions from all models are **combined (usually weighted)** to form the final strong model.\n",
    "\n",
    "---\n",
    "\n",
    "## **How Boosting Works (Step-by-Step)**\n",
    "\n",
    "1. **Initialize Weights:**  \n",
    "   - Assign equal weights to all training samples.\n",
    "\n",
    "2. **Train Weak Learner:**  \n",
    "   - Train a simple base model (e.g., a small decision tree) on the data.\n",
    "\n",
    "3. **Evaluate Errors:**  \n",
    "   - Check which samples were classified incorrectly by the model.\n",
    "\n",
    "4. **Update Weights:**  \n",
    "   - Increase weights of misclassified samples so that the next model focuses more on difficult cases.\n",
    "\n",
    "5. **Combine Models:**  \n",
    "   - After several rounds, combine all the weak learners into a single **strong learner**, usually using a weighted voting or weighted sum approach.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6e815d-af56-4cf9-8c9e-c019c8810e6f",
   "metadata": {},
   "source": [
    "## Question 2: What is the difference between AdaBoost and Gradient Boosting in terms of how models are trained?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ab2586-f4c6-4483-aa85-2d4b3d3bd4d3",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## **Introduction**\n",
    "\n",
    "Both **AdaBoost** (Adaptive Boosting) and **Gradient Boosting** are popular **boosting algorithms** used in ensemble learning.  \n",
    "Although they share the same goal ‚Äî **to combine multiple weak learners** (like shallow decision trees) into a strong predictive model ‚Äî  \n",
    "they differ in **how they train** and **update** their weak learners during the boosting process.\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Core Idea**\n",
    "\n",
    "| Aspect | **AdaBoost (Adaptive Boosting)** | **Gradient Boosting** |\n",
    "|:-------|:---------------------------------|:----------------------|\n",
    "| **Main Principle** | Focuses on **adjusting sample weights** based on previous errors. | Focuses on **reducing residual errors** (gradients) from the previous model. |\n",
    "| **Objective** | Emphasizes **misclassified samples** by increasing their weights. | Fits the next model to the **residuals (errors)** of the previous model. |\n",
    "\n",
    "---\n",
    "\n",
    "## **2. How Models Are Trained (Step-by-Step)**\n",
    "\n",
    "### **‚û° AdaBoost Training Process**\n",
    "1. **Initialize sample weights** equally for all training examples.  \n",
    "2. **Train a weak learner** (e.g., a decision stump).  \n",
    "3. **Compute error** for that model ‚Äî find how many samples were misclassified.  \n",
    "4. **Increase weights** of misclassified samples so that the next learner focuses more on difficult cases.  \n",
    "5. **Compute model weight (Œ±)** based on its accuracy ‚Äî better models get higher influence in the final prediction.  \n",
    "6. **Combine models** using a **weighted majority vote** (for classification) or **weighted average** (for regression).\n",
    "\n",
    "**Key Idea:**  \n",
    "AdaBoost works by **re-weighting samples**, not by fitting to residuals.\n",
    "\n",
    "---\n",
    "\n",
    "### **‚û° Gradient Boosting Training Process**\n",
    "1. **Start with an initial prediction** (like the mean of the target values).  \n",
    "2. **Compute residuals** ‚Äî the difference between actual values and the current model‚Äôs predictions.  \n",
    "3. **Train the next weak learner** to predict these residuals (errors).  \n",
    "4. **Add the new model‚Äôs predictions** (scaled by a learning rate) to the ensemble.  \n",
    "5. **Repeat** until the model performance stops improving or a set number of learners is reached.\n",
    "\n",
    " **Key Idea:**  \n",
    "Gradient Boosting works by **fitting new models to the residual errors** ‚Äî effectively performing gradient descent in function space.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Error Handling**\n",
    "\n",
    "| Aspect | **AdaBoost** | **Gradient Boosting** |\n",
    "|:-------|:--------------|:----------------------|\n",
    "| **Error focus** | Increases weights of misclassified samples. | Minimizes the residual (error) using gradient descent. |\n",
    "| **Loss function** | Exponential loss (for classification). | Can use any differentiable loss function (e.g., Mean Squared Error, Log Loss). |\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Model Combination**\n",
    "\n",
    "- **AdaBoost:**  \n",
    "  Combines models using **weighted voting** (classification) or **weighted sum** (regression).\n",
    "\n",
    "- **Gradient Boosting:**  \n",
    "  Combines models **additively** by summing up all predictions (each scaled by a learning rate).\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Learning Rate**\n",
    "\n",
    "- **AdaBoost:**  \n",
    "  Implicitly controlled through model weights ‚Äî no explicit learning rate parameter.  \n",
    "- **Gradient Boosting:**  \n",
    "  Includes an explicit **learning rate (shrinkage parameter)** that controls how much each new model contributes.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91cceb1-20c0-4c46-aa4e-4f0c0756615d",
   "metadata": {},
   "source": [
    "## Question 3: How does regularization help in XGBoost?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1033b56a-8112-434c-b2d2-421f57c9a479",
   "metadata": {},
   "source": [
    "# Question 3: How Does Regularization Help in XGBoost?\n",
    "\n",
    "---\n",
    "\n",
    "## **Introduction**\n",
    "\n",
    "**XGBoost (Extreme Gradient Boosting)** is an advanced implementation of gradient boosting that is designed for **speed, performance, and control over overfitting**.  \n",
    "One of the major reasons XGBoost performs so well compared to traditional gradient boosting is its built-in support for **regularization**.\n",
    "\n",
    "Regularization in XGBoost helps improve **model generalization** by preventing the model from becoming too complex and overfitting to training data.\n",
    "\n",
    "---\n",
    "\n",
    "## **1. What is Regularization?**\n",
    "\n",
    "**Regularization** is a technique used in machine learning to **penalize model complexity**.  \n",
    "It adds a **penalty term** to the model‚Äôs objective function so that the model not only minimizes prediction errors but also keeps the learned parameters simple and small.\n",
    "\n",
    "In simpler terms:  \n",
    "> Regularization discourages the model from fitting noise and forces it to focus on the most important patterns.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. XGBoost Objective Function**\n",
    "\n",
    "XGBoost optimizes the following **regularized objective function**:\n",
    "\n",
    "$$\n",
    "\\text{Obj} = \\sum_{i=1}^{n} l(y_i, \\hat{y}_i) + \\sum_{k=1}^{K} \\Omega(f_k)\n",
    "$$\n",
    "\n",
    "**where:**\n",
    "- \\( l(y_i, \\hat{y}_i) \\): Loss function (e.g., mean squared error, log loss)  \n",
    "- \\( \\Omega(f_k) \\): Regularization term for each tree \\( f_k \\)  \n",
    "- \\( K \\): Number of trees in the model  \n",
    "\n",
    "The regularization term is defined as:\n",
    "\n",
    "$$\n",
    "\\Omega(f) = \\gamma T + \\frac{1}{2} \\lambda \\sum_{j=1}^{T} w_j^2\n",
    "$$\n",
    "\n",
    "**Here:**\n",
    "- \\( T \\): number of leaves (terminal nodes) in the tree  \n",
    "- \\( w_j \\): weight (score) assigned to leaf \\( j \\)  \n",
    "- \\( \\gamma \\): penalty for the number of leaves (controls tree depth)  \n",
    "- \\( \\lambda \\): L2 regularization term (controls weight magnitude)\n",
    ")\n",
    "\n",
    "\n",
    "## **3. Types of Regularization in XGBoost**\n",
    "\n",
    "### **(a) L2 Regularization (Ridge Regularization)**\n",
    "- Represented by **Œª (lambda)**.  \n",
    "- Penalizes large leaf weights by adding the term \\( \\frac{1}{2} \\lambda \\sum w_j^2 \\).  \n",
    "- Helps smooth the model and reduces variance.\n",
    "\n",
    "üß† *Effect:* Prevents any single leaf from having a dominant influence and keeps the model stable.\n",
    "\n",
    "---\n",
    "\n",
    "### **(b) L1 Regularization (Lasso Regularization)**\n",
    "- Represented by **Œ± (alpha)**.  \n",
    "- Penalizes the absolute value of leaf weights: \\( \\alpha \\sum |w_j| \\).  \n",
    "- Encourages sparsity ‚Äî pushes some leaf weights to **zero**, effectively performing **feature selection**.\n",
    "\n",
    "üß† *Effect:* Simplifies the model by ignoring unimportant splits or features.\n",
    "\n",
    "---\n",
    "\n",
    "### **(c) Tree Complexity Penalty (Œ≥ ‚Äì Gamma)**\n",
    "- Adds a fixed penalty for each additional leaf in the tree.  \n",
    "- Controls tree **depth** and **complexity**.  \n",
    "- A larger Œ≥ value means the algorithm will only add a leaf if it **improves the model significantly**.\n",
    "\n",
    "üß† *Effect:* Prevents overfitting by stopping unnecessary growth of trees.\n",
    "\n",
    "---\n",
    "\n",
    "## **4. How Regularization Helps XGBoost**\n",
    "\n",
    "| **Aspect** | **Without Regularization** | **With Regularization** |\n",
    "|-------------|-----------------------------|---------------------------|\n",
    "| **Model Complexity** | Can create very deep trees that overfit | Penalizes deep or overly complex trees |\n",
    "| **Generalization** | May fit noise in training data | Learns patterns that generalize better |\n",
    "| **Feature Selection** | Uses all features | Ignores less important features (via L1) |\n",
    "| **Stability** | Sensitive to small data changes | More stable and robust predictions |\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Parameters Related to Regularization in XGBoost**\n",
    "\n",
    "| Parameter | Type | Purpose |\n",
    "|:-----------|:------|:----------|\n",
    "| `lambda` | L2 Regularization | Reduces large leaf weights (default = 1) |\n",
    "| `alpha` | L1 Regularization | Encourages sparsity and feature selection (default = 0) |\n",
    "| `gamma` | Tree Complexity Penalty | Controls how much gain a split must achieve to be added (default = 0) |\n",
    "| `max_depth` | Structural Constraint | Limits tree depth to prevent overfitting |\n",
    "| `min_child_weight` | Structural Constraint | Minimum sum of instance weights in a leaf; prevents overly specific splits |\n",
    "\n",
    "---\n",
    "\n",
    "## **6. Example Explanation**\n",
    "\n",
    "Suppose XGBoost is building a model to predict loan defaults:\n",
    "- Without regularization, it may create a **very deep tree** that perfectly fits the training data, including noise.  \n",
    "- With regularization:\n",
    "  - The **L2 penalty** keeps leaf weights smaller, preventing extreme predictions.  \n",
    "  - The **gamma penalty** stops new branches unless they offer significant improvement.  \n",
    "  - The **L1 penalty** eliminates weak or irrelevant features.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7495bb-6516-4743-b571-b80e36c44f2d",
   "metadata": {},
   "source": [
    "## Question 4: Why is CatBoost considered efficient for handling categorical data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fba264-aeaa-4110-bd50-51ae104f5e76",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "## **Introduction**\n",
    "\n",
    "**CatBoost** (short for *Categorical Boosting*) is a high-performance **gradient boosting algorithm** developed by **Yandex** that is specifically designed to handle **categorical data efficiently**.  \n",
    "In many real-world datasets, categorical variables such as *‚ÄúGender,‚Äù ‚ÄúCountry,‚Äù ‚ÄúEducation Level,‚Äù ‚ÄúProduct Type,‚Äù* etc., play a major role in determining the output.  \n",
    "Traditional machine learning algorithms often require **preprocessing** steps like **label encoding** or **one-hot encoding**, which can lead to **information loss**, **high dimensionality**, and **overfitting**.  \n",
    "\n",
    "CatBoost, however, is unique because it can **natively process categorical features** without requiring extensive preprocessing. This built-in capability, combined with its innovative **ordered boosting** and **target statistics encoding**, makes it one of the most powerful algorithms for datasets containing categorical variables.\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Native Handling of Categorical Variables**\n",
    "\n",
    "### **Traditional Problem:**\n",
    "In most machine learning algorithms (like XGBoost, LightGBM, or Random Forest), categorical variables must first be converted into numerical form since algorithms cannot process text or categories directly.\n",
    "\n",
    "Common methods include:\n",
    "- **Label Encoding** ‚Üí Assigning an integer to each category (e.g., ‚ÄúMale‚Äù = 0, ‚ÄúFemale‚Äù = 1)\n",
    "- **One-Hot Encoding** ‚Üí Creating binary columns for each category\n",
    "\n",
    "However, these approaches have drawbacks:\n",
    "- **Label Encoding** may imply a false ordinal relationship (e.g., ‚ÄúRed‚Äù < ‚ÄúBlue‚Äù < ‚ÄúGreen‚Äù)\n",
    "- **One-Hot Encoding** increases the **dimensionality** of the dataset, leading to **memory inefficiency** and slower training.\n",
    "\n",
    "### **CatBoost‚Äôs Approach:**\n",
    "CatBoost allows users to directly input categorical features into the model. The algorithm automatically converts these features internally using **efficient statistical methods**, eliminating the need for any manual encoding step.  \n",
    "This results in:\n",
    "- **Simpler preprocessing**\n",
    "- **Reduced training time**\n",
    "- **More accurate representation** of the underlying data relationships\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Ordered Target Statistics (Mean Encoding without Leakage)**\n",
    "\n",
    "CatBoost replaces categorical values with numerical statistics that represent their relationship with the target variable. However, unlike regular *mean encoding*, which can cause **target leakage**, CatBoost uses a unique technique called **Ordered Target Statistics**.\n",
    "\n",
    "### **How it Works:**\n",
    "- CatBoost first creates several **random permutations** of the training data.\n",
    "- For each categorical feature and permutation, it computes the **average target value** for that feature using **only the samples that come before** the current one in the permutation.\n",
    "- This ensures that **no information from the future (or target leakage)** influences the encoding of the current sample.\n",
    "\n",
    "## **3. Combination of Categorical Features**\n",
    "\n",
    "Many categorical variables interact in complex ways.  \n",
    "For example:\n",
    "- The relationship between **‚ÄúCity‚Äù** and **‚ÄúProduct Type‚Äù** might affect the target variable.\n",
    "- A simple one-hot encoding would not capture such interactions automatically.\n",
    "\n",
    "CatBoost addresses this by automatically creating **combinations of categorical features** (known as *feature crosses*) during training.  \n",
    "It evaluates and constructs new features like:\n",
    "- (‚ÄúCity‚Äù, ‚ÄúProduct Type‚Äù)\n",
    "- (‚ÄúGender‚Äù, ‚ÄúEducation Level‚Äù), etc.\n",
    "\n",
    "These interactions allow CatBoost to:\n",
    "- Capture **non-linear relationships** between features\n",
    "- Enhance **model expressiveness**\n",
    "- Reduce the need for **manual feature engineering**\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Handling High-Cardinality Categorical Features**\n",
    "\n",
    "High-cardinality features are categorical columns with **many unique categories**, such as ‚ÄúCustomer ID‚Äù or ‚ÄúProduct Code.‚Äù  \n",
    "Traditional one-hot encoding would create **thousands of binary columns**, consuming excessive memory and computational power.\n",
    "\n",
    "CatBoost overcomes this challenge using:\n",
    "- **Statistical encoding with hashing** to compress high-cardinality features efficiently\n",
    "- **Ordered statistics** to ensure that each category‚Äôs numerical representation is **meaningful and stable**\n",
    "\n",
    "Thus, CatBoost can train effectively even on datasets with **millions of categorical levels**.\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Ordered Boosting: Preventing Overfitting**\n",
    "\n",
    "In addition to ordered statistics for encoding, CatBoost also introduces **ordered boosting**, a novel variant of gradient boosting.\n",
    "\n",
    "### **Problem in Traditional Boosting:**\n",
    "Standard boosting algorithms may use the same dataset for both:\n",
    "- Calculating gradients (errors)\n",
    "- Fitting the next model (trees)\n",
    "\n",
    "This can lead to **prediction shift**, where the model learns from its own prediction errors incorrectly, resulting in overfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad2113d-2181-4b58-ba3c-a63a333f89ee",
   "metadata": {},
   "source": [
    "## Question 5: What are some real-world applications where boosting techniques are preferred over bagging methods?\n",
    "# Datasets:\n",
    "## ‚óè Use sklearn.datasets.load_breast_cancer() for classification tasks.\n",
    "## ‚óè Use sklearn.datasets.fetch_california_housing() for regression tasks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e4ba2c-8e2a-4cac-b6da-3abf4fc5e908",
   "metadata": {},
   "source": [
    "# **Question 5: What Are Some Real-World Applications Where Boosting Techniques Are Preferred Over Bagging Methods?**\n",
    "\n",
    "---\n",
    "\n",
    "## **Introduction**\n",
    "\n",
    "**Ensemble learning** combines multiple weak learners (models) to form a strong predictive model.  \n",
    "Two popular ensemble methods are **Bagging** (Bootstrap Aggregating) and **Boosting**.\n",
    "\n",
    "While **Bagging** (like Random Forest) focuses on **reducing variance** by training models independently on random subsets of data,  \n",
    "**Boosting** (like AdaBoost, Gradient Boosting, XGBoost, LightGBM, CatBoost) focuses on **reducing bias** by sequentially training models that correct the errors of previous models.\n",
    "\n",
    "Boosting techniques are preferred in many real-world applications because they provide:\n",
    "- **Higher accuracy**\n",
    "- **Better generalization**\n",
    "- **Strong performance on complex, non-linear datasets**\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Key Difference Between Bagging and Boosting**\n",
    "\n",
    "| Feature | **Bagging (e.g., Random Forest)** | **Boosting (e.g., XGBoost, AdaBoost)** |\n",
    "|----------|------------------------------------|-----------------------------------------|\n",
    "| **Goal** | Reduce variance | Reduce bias and variance |\n",
    "| **Training** | Models trained independently | Models trained sequentially |\n",
    "| **Error Handling** | Equal weight to all samples | Focuses on misclassified samples |\n",
    "| **Overfitting Risk** | Lower | Slightly higher (but controlled by regularization) |\n",
    "| **Speed** | Faster | Slower but more accurate |\n",
    "| **Best For** | Simple tasks, large datasets | Complex, imbalanced, and structured datasets |\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Why Boosting is Preferred in Certain Applications**\n",
    "\n",
    "Boosting techniques are particularly favored when:\n",
    "- The dataset is **complex and non-linear**.\n",
    "- Some classes are **harder to classify**.\n",
    "- **High predictive accuracy** is critical (e.g., healthcare, finance).\n",
    "- There are **structured/tabular datasets** with both numerical and categorical variables.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Real-World Applications of Boosting Techniques**\n",
    "\n",
    "### **A. Healthcare ‚Äî Disease Detection (Classification Task)**\n",
    "\n",
    "- **Dataset Example:** `sklearn.datasets.load_breast_cancer()`\n",
    "- **Goal:** Classify whether a tumor is **malignant** or **benign**.\n",
    "- **Preferred Algorithm:** XGBoost, CatBoost, or AdaBoost.\n",
    "\n",
    "#### **Why Boosting is Better:**\n",
    "- Medical data often contains **imbalanced classes** (e.g., more benign cases than malignant).  \n",
    "- Boosting assigns **higher weights to misclassified cases**, improving accuracy on minority classes.\n",
    "- Provides **explainability** using feature importance (helpful for doctors).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fd4f91f-53f6-4f69-aad5-48c1f7c0edca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.956140350877193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [22:17:45] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Boosting model\n",
    "model = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "368a44f0-a232-4918-a0e6-cf327b89894a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.47954250381653163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Load dataset\n",
    "data = fetch_california_housing()\n",
    "X, y = data.data, data.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Boosting Regressor\n",
    "model = XGBRegressor(n_estimators=500, learning_rate=0.05, max_depth=4)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"RMSE:\", mean_squared_error(y_test, y_pred, squared=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10678b3-5f5e-4fad-bea9-57a021841e4e",
   "metadata": {},
   "source": [
    "## Question 6: Write a Python program to:\n",
    "## ‚óè Train an AdaBoost Classifier on the Breast Cancer dataset\n",
    "## ‚óè Print the model accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d3d7414-af18-40a3-b31e-04ea11279bfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ AdaBoost Classifier Accuracy on Breast Cancer Dataset: 97.37%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Initialize and train AdaBoost model\n",
    "model = AdaBoostClassifier(n_estimators=100, learning_rate=0.8, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Print the accuracy\n",
    "print(\"‚úÖ AdaBoost Classifier Accuracy on Breast Cancer Dataset: {:.2f}%\".format(accuracy * 100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d697e14-b015-43f2-9850-63b49bb7ebed",
   "metadata": {},
   "source": [
    "## Question 7: Write a Python program to:\n",
    "## ‚óè Train a Gradient Boosting Regressor on the California Housing dataset\n",
    "## ‚óè Evaluate performance using R-squared score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8f3a2be-6bc8-490e-a0da-49175e917ce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Gradient Boosting Regressor Performance:\n",
      "R-squared Score: 0.8185\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Load dataset\n",
    "data = fetch_california_housing()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Initialize and train the Gradient Boosting Regressor\n",
    "model = GradientBoostingRegressor(\n",
    "    n_estimators=200,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=4,\n",
    "    random_state=42\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Print results\n",
    "print(\"‚úÖ Gradient Boosting Regressor Performance:\")\n",
    "print(\"R-squared Score: {:.4f}\".format(r2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a9a9f7-8a55-4a9c-9eaa-4376526e0059",
   "metadata": {},
   "source": [
    "## Question 8: Write a Python program to:\n",
    "## ‚óè Train an XGBoost Classifier on the Breast Cancer dataset\n",
    "## ‚óè Tune the learning rate using GridSearchCV\n",
    "## ‚óè Print the best parameters and accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3fb0696-6a98-45a1-a424-98e15f1adefc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Best Parameters: {'learning_rate': 0.3}\n",
      "‚úÖ Model Accuracy on Test Set: 95.61%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from xgboost import XGBClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load dataset\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Define XGBoost model\n",
    "xgb_model = XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=3,\n",
    "    random_state=42,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "\n",
    "# Define parameter grid for learning rate tuning\n",
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2, 0.3]\n",
    "}\n",
    "\n",
    "# Set up GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=xgb_model,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Train the model with grid search\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and accuracy\n",
    "print(\"‚úÖ Best Parameters:\", grid_search.best_params_)\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"‚úÖ Model Accuracy on Test Set: {:.2f}%\".format(accuracy * 100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f0d393-9529-4ca2-90f6-ffcd9f20dbdb",
   "metadata": {},
   "source": [
    "## Question 9: Write a Python program to:\n",
    "## ‚óè Train a CatBoost Classifier\n",
    "## ‚óè Plot the confusion matrix using seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e09b9b46-00d3-4037-9830-822827825eff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ CatBoost Classifier Accuracy: 96.49%\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load dataset\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Initialize and train CatBoost model\n",
    "model = CatBoostClassifier(\n",
    "    iterations=200,\n",
    "    learning_rate=0.1,\n",
    "    depth=6,\n",
    "    loss_function='Logloss',\n",
    "    verbose=0,\n",
    "    random_state=42\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"‚úÖ CatBoost Classifier Accuracy: {:.2f}%\".format(accuracy * 100))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Greens',\n",
    "            xticklabels=data.target_names,\n",
    "            yticklabels=data.target_names)\n",
    "plt.title(\"Confusion Matrix - CatBoost Classifier\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70dca08c-af21-4d82-be65-94271a2c76e7",
   "metadata": {},
   "source": [
    "## Question 10: You're working for a FinTech company trying to predict loan default using customer demographics and transaction behavior. The dataset is imbalanced, contains missing values, and has both numeric and categorical features.\n",
    "# Describe your step-by-step data science pipeline using boosting techniques:\n",
    "## ‚óè Data preprocessing & handling missing/categorical values\n",
    "## ‚óè Choice between AdaBoost, XGBoost, or CatBoost\n",
    "## ‚óè Hyperparameter tuning strategy\n",
    "## ‚óè Evaluation metrics you'd choose and why\n",
    "## ‚óè How the business would benefit from your model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10485d2c-ea09-49f9-8a1e-d91ef2f26c43",
   "metadata": {},
   "source": [
    "# 1) Problem framing & high-level approach\n",
    "\n",
    "Objective: Predict whether a customer will default on a loan (binary classification).\n",
    "\n",
    "Constraints: Imbalanced target (defaults << non-defaults), missing values, mixed numeric + categorical features, regulatory/privacy concerns.\n",
    "\n",
    "High-level plan: Build an end-to-end pipeline that: cleans data ‚Üí engineers features ‚Üí handles imbalance ‚Üí trains + tunes a boosting model ‚Üí evaluates using business-aware metrics ‚Üí deploys with monitoring, interpretability and fairness checks.\n",
    "\n",
    "# 2) Data preprocessing & handling missing / categorical values\n",
    "2.1 Initial checks & housekeeping\n",
    "\n",
    "Inspect distributions, data types, missingness patterns, unique counts for categorical columns, outliers.\n",
    "\n",
    "Split data early into train / validation / test (e.g., 60/20/20) using stratified split on the target to preserve imbalance. Do this before heavy preprocessing to avoid leakage.\n",
    "\n",
    "## 2.2 Missing values\n",
    "\n",
    "Explore whether missingness is MCAR / MAR / MNAR.\n",
    "\n",
    "Simple rules:\n",
    "\n",
    "If a column has very high missingness (e.g., > 60‚Äì80%) consider dropping or deriving a missingness indicator.\n",
    "\n",
    "For numeric features: impute using median (robust) or model-based imputation (KNN / IterativeImputer) if justified.\n",
    "\n",
    "For categorical features: impute with a new category like \"MISSING\" (so model can learn missingness signal).\n",
    "\n",
    "Important: When imputing, fit imputers only on training data and apply to val/test.\n",
    "\n",
    "## 2.3 Categorical features\n",
    "\n",
    "Two options:\n",
    "\n",
    "CatBoost: pass categorical column names/indices directly and no encoding required ‚Äî CatBoost uses ordered target statistics internally and handles high-cardinality effectively.\n",
    "\n",
    "XGBoost / AdaBoost: must encode. Prefer:\n",
    "\n",
    "Target/mean encoding with smoothing (carefully, with K-fold or out-of-fold to avoid leakage) for high-cardinality features.\n",
    "\n",
    "One-hot for low-cardinality (< ~10 levels).\n",
    "\n",
    "Frequency encoding or hashing when many categories.\n",
    "\n",
    "Example of out-of-fold mean encoding (if using XGBoost): always compute encodings using training folds, apply to validation/test.\n",
    "\n",
    "## 2.4 Feature engineering ideas (domain-specific)\n",
    "\n",
    "Transaction behavior features: recent delinquency counts, avg monthly balance, max drawdown, velocity (transactions per day), time since last default/payment delay.\n",
    "\n",
    "Aggregations: rolling means, decayed averages, transaction category ratios.\n",
    "\n",
    "Interaction terms: e.g., income / monthly_payment, age * delinquency_rate.\n",
    "\n",
    "Time features: days since account opening, seasonality flags.\n",
    "\n",
    "Missingness indicators: is_alt_contact_missing, num_features_missing.\n",
    "\n",
    "## 2.5 Scaling\n",
    "\n",
    "Tree-based models (boosting) don‚Äôt need standard scaling for accuracy, but scaling can help interpretability or models used later (e.g., logistic baseline).\n",
    "\n",
    "Save preprocessing pipeline (imputers, encoders, scalers) for production.\n",
    "\n",
    "# 3) Choice between AdaBoost, XGBoost, or CatBoost\n",
    "Recommendation (summary)\n",
    "\n",
    "Primary choice: CatBoost ‚Äî best when many categorical features (native handling, ordered encoding, robust to target leakage). Good default for tabular FinTech data.\n",
    "\n",
    "Runner-up: XGBoost / LightGBM ‚Äî extremely fast and well-tuned; use with careful categorical encoding (target encoding out-of-fold) and regularization. LightGBM is very fast for very large datasets.\n",
    "\n",
    "AdaBoost ‚Äî historically important, but less flexible and often outperformed by more modern gradient boosting frameworks (XGBoost/LightGBM/CatBoost). Use only for simple baselines or tiny datasets.\n",
    "\n",
    "Rationale\n",
    "\n",
    "FinTech data = many categorical variables (e.g., occupation code, merchant id), skewed distributions, possible high cardinality; CatBoost‚Äôs ordered target stats + categorical combinations reduce manual work and leakage risk.\n",
    "\n",
    "XGBoost provides excellent control of regularization and speed; use when you need finer performance tuning and when you've prepared categorical encodings carefully.\n",
    "\n",
    "# 5) Handling imbalanced data (class imbalance strategies)\n",
    "\n",
    "Model-level:\n",
    "\n",
    "Use scale_pos_weight (XGBoost) or class_weights (CatBoost supports auto_class_weights='Balanced' or manual weights).\n",
    "\n",
    "Use objective or eval metric adapted for class imbalance (AUC, PR AUC).\n",
    "\n",
    "Data-level:\n",
    "\n",
    "Careful use of oversampling (SMOTE variants) on training set only ‚Äî boosting models can be sensitive; sometimes simple class weighting is safer.\n",
    "\n",
    "Undersampling may discard valuable majority-class data ‚Äî use only if dataset huge.\n",
    "\n",
    "Hybrid: small, targeted oversampling + class weights.\n",
    "\n",
    "Threshold tuning: after predicting probabilities, choose operating point to balance business costs (precision/recall tradeoff) rather than default 0.5.\n",
    "\n",
    "# 6) Hyperparameter tuning strategy\n",
    "## 6.1 What to tune (suggested)\n",
    "\n",
    "learning_rate (eta): 0.01 ‚Äì 0.3\n",
    "\n",
    "n_estimators / iterations: large with early stopping (500‚Äì5000)\n",
    "\n",
    "max_depth / depth: 3‚Äì10\n",
    "\n",
    "l2_leaf_reg (CatBoost) or reg_lambda/reg_alpha (XGBoost): regularization\n",
    "\n",
    "subsample (row sampling), colsample_bytree (feature sampling)\n",
    "\n",
    "min_child_weight / min_data_in_leaf\n",
    "\n",
    "For CatBoost: bagging_temperature and random_strength\n",
    "\n",
    "## 6.2 Practical search flow\n",
    "\n",
    "Baseline + early stopping. Train a default model with early stopping to get baseline and appropriate number of iterations.\n",
    "\n",
    "Coarse search (RandomizedSearch or Bayesian) over wide ranges to find promising areas (fewer evaluations, good coverage).\n",
    "\n",
    "Refine with Bayesian optimization (e.g., Optuna, Hyperopt) or smaller GridSearch around best region.\n",
    "\n",
    "Final fine-tuning: adjust learning_rate + increase n_estimators while monitoring validation AUC and overfitting.\n",
    "\n",
    "Always use time/stratified CV appropriate for data (if temporal data, use time-based split; otherwise stratified k-fold).\n",
    "\n",
    "## 6.3 Practical tips\n",
    "\n",
    "Use AUC/PR AUC for scoring during search for imbalanced tasks (more on metrics below).\n",
    "\n",
    "Use early stopping to avoid overfitting and to speed up tuning.\n",
    "\n",
    "Use nested CV or a held-out test set to estimate generalization.\n",
    "\n",
    "Use GPU for faster tuning if available.\n",
    "\n",
    "# 7) Evaluation metrics (and why)\n",
    "\n",
    "Because the dataset is imbalanced, accuracy is misleading. Use a mix of probability-based, classwise and business-aware metrics:\n",
    "\n",
    "Primary metrics\n",
    "\n",
    "ROC AUC (Area Under ROC): overall separability measure, robust to class imbalance in terms of ranking, but can overestimate performance when classes are extremely imbalanced.\n",
    "\n",
    "PR AUC (Area Under Precision-Recall curve): more informative when positive class is rare; focuses on performance for the positive (default) class.\n",
    "\n",
    "Secondary / operational metrics\n",
    "\n",
    "Precision / Recall / F1 at chosen threshold: choose threshold to meet business recall/precision tradeoff.\n",
    "\n",
    "Recall (sensitivity): proportion of actual defaulters caught ‚Äî critical if missing defaulters is costly.\n",
    "\n",
    "Precision: proportion of flagged defaulters who actually default ‚Äî important to avoid false alarms and customer friction.\n",
    "\n",
    "Confusion Matrix: raw counts for TP/FP/TN/FN.\n",
    "\n",
    "Specificity (True Negative Rate) if false positives have costs.\n",
    "\n",
    "Calibration (Brier score / calibration curve): ensures predicted probabilities match observed frequencies ‚Äî important if probabilities are used in pricing decisions or risk scoring.\n",
    "\n",
    "Cost-sensitive metrics: Define a business cost matrix (cost_fn * FN + cost_fp * FP) and compute expected cost (preferred approach ‚Äî directly optimizes what the business cares about).\n",
    "\n",
    "# 8) Model interpretability & fairness\n",
    "Interpretability\n",
    "\n",
    "Feature importance (CatBoost / XGBoost built-in).\n",
    "\n",
    "SHAP values for local & global explanations ‚Äî crucial for regulatory compliance and trust in FinTech.\n",
    "\n",
    "Provide model explanations for rejected applicants (e.g., top contributing features).\n",
    "\n",
    "Fairness & Bias Checks\n",
    "\n",
    "Check performance (AUC, FPR/FNR) across demographic groups (age, gender, location) and ensure compliance with fair lending laws.\n",
    "\n",
    "If disparities found, investigate data, re-balance, or use fairness-aware methods and document rationale.\n",
    "\n",
    "# 9) Threshold selection and business decisioning\n",
    "\n",
    "Choose operating threshold using validation set or cost-based analysis.\n",
    "\n",
    "Example approaches:\n",
    "\n",
    "Maximize F1 if want balance.\n",
    "\n",
    "Maximize Recall subject to minimum Precision (business rule).\n",
    "\n",
    "Minimize expected monetary loss using cost matrix.\n",
    "\n",
    "# 10) Deployment, monitoring & feedback loop\n",
    "Pre-deployment\n",
    "\n",
    "Store model artifacts + preprocessing pipeline, versioned.\n",
    "\n",
    "Save: model weights, feature list, encoders, training data schema.\n",
    "\n",
    "Expose endpoint with prediction + probability + top-3 SHAP reasons.\n",
    "\n",
    "Monitoring (post-deployment)\n",
    "\n",
    "Data drift: monitor feature distributions (population stability index).\n",
    "\n",
    "Model drift / performance: re-evaluate AUC/PR AUC on recent labeled data; track calibration shift.\n",
    "\n",
    "Business KPIs: delinquency rates, approval rates, realized loss vs predicted risk.\n",
    "\n",
    "Retraining policy: scheduled (monthly / quarterly) or triggered by drift/performance drop.\n",
    "\n",
    "# 11) How the business benefits\n",
    "Direct financial benefits\n",
    "\n",
    "Reduced credit losses: better identification of likely defaulters reduces bad loans and associated provisioning.\n",
    "\n",
    "Improved approval decisions: increase acceptance of low-risk borrowers (higher revenue) and stricter evaluation for high-risk.\n",
    "\n",
    "Optimized pricing: use calibrated probabilities to set interest rates aligned to risk (risk-based pricing).\n",
    "\n",
    "Operational benefits\n",
    "\n",
    "Automation & speed: automated risk scoring speeds decisions and lowers manual review costs.\n",
    "\n",
    "Better segmentation: create targeted interest/collections strategies (early intervention for borderline cases).\n",
    "\n",
    "Resource allocation: focus collections and monitoring effort on accounts with high predicted default probability.\n",
    "\n",
    "Compliance & reputation\n",
    "\n",
    "Explainability (SHAP + feature importance) enables regulatory audits and reduces compliance risk.\n",
    "\n",
    "Fairness checks reduce potential bias/legal exposure.\n",
    "\n",
    "Strategic benefits\n",
    "\n",
    "Competitive advantage: more accurate risk models enable aggressive but safe market expansion.\n",
    "\n",
    "Product personalization: combine risk with profitability to tailor loan products.\n",
    "\n",
    "# 12) Quick end-to-end checklist (actionable)\n",
    "\n",
    "Split data (stratified) into train/val/test.\n",
    "\n",
    "EDA & missingness analysis; create missing flags.\n",
    "\n",
    "Impute (train-only) and engineer features (time windows, aggregates).\n",
    "\n",
    "If using XGBoost, create out-of-fold mean encodings for categorical vars. If using CatBoost, pass categorical indices.\n",
    "\n",
    "Train baseline CatBoost with early stopping, monitor AUC/PR AUC.\n",
    "\n",
    "Handle imbalance with class weights or conservative oversampling + threshold tuning.\n",
    "\n",
    "Hyperparameter tune with Random/ Bayesian search (optuna), optimizing PR AUC or business cost.\n",
    "\n",
    "Evaluate: ROC AUC, PR AUC, precision/recall at business threshold, calibration.\n",
    "\n",
    "Explain with SHAP, export explanations for top decisions.\n",
    "\n",
    "Deploy, monitor drift, and retrain as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ea547e-78e4-484e-bcb0-b4ff89239108",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
