{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "037ccf16-0158-45c3-901e-5cbcd70fc242",
   "metadata": {},
   "source": [
    "## Question 1: What is a Decision Tree, and how does it work in the context of classification?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32299ea4-3268-4b80-a43b-e76c7ddee399",
   "metadata": {},
   "source": [
    "### **Definition**\n",
    "\n",
    "A **Decision Tree** is a **supervised machine learning algorithm** that is widely used for **classification and regression** tasks.  \n",
    "In the context of **classification**, a decision tree predicts the **class label** of an input by learning **simple decision rules** inferred from data features.\n",
    "\n",
    "It is called a \"tree\" because it has a structure similar to a tree in nature — consisting of **nodes**, **branches**, and **leaves**.\n",
    "\n",
    "\n",
    "\n",
    "### **Structure of a Decision Tree**\n",
    "\n",
    "- **Root Node:**  \n",
    "  Represents the entire dataset and the first feature used to make a decision.\n",
    "\n",
    "- **Decision Nodes:**  \n",
    "  These are internal nodes that split the data based on certain conditions (e.g., “Is Age > 30?”).\n",
    "\n",
    "- **Branches:**  \n",
    "  Represent the outcomes of decisions and connect nodes.\n",
    "\n",
    "- **Leaf Nodes:**  \n",
    "  Represent the final class label (e.g., “Yes” or “No”) or output value.\n",
    "\n",
    "\n",
    "\n",
    "### **How a Decision Tree Works (Classification Process)**\n",
    "\n",
    "1. **Data Splitting:**  \n",
    "   The algorithm selects the feature that best divides the data into distinct classes.  \n",
    "   This is usually done using measures like:\n",
    "   - **Gini Impurity**\n",
    "   - **Entropy / Information Gain**\n",
    "   - **Gain Ratio**\n",
    "\n",
    "2. **Decision Making:**  \n",
    "   Each node splits the data based on a condition — for example:\n",
    "\n",
    "\n",
    "\n",
    "3. **Recursive Splitting:**  \n",
    "The process continues recursively — splitting each subset into smaller groups until:\n",
    "- All data points in a node belong to the same class, or\n",
    "- A stopping criterion is met (e.g., maximum depth reached).\n",
    "\n",
    "4. **Prediction:**  \n",
    "For a new data point, the algorithm follows the path of decisions from the root to a leaf node, where it assigns the final class label.\n",
    "\n",
    "\n",
    "\n",
    "### **Example**\n",
    "\n",
    "Suppose we want to predict whether a person will **buy a car** based on two features: **Age** and **Income**.\n",
    "\n",
    "A simple decision tree might look like:\n",
    "\n",
    "\n",
    "         [Age > 30?]\n",
    "          /       \\\n",
    "      Yes/         \\No\n",
    "      [Income > 50k?]  → No\n",
    "       /       \\\n",
    "    Yes/         \\No\n",
    "    Buy           No Buy\n",
    "\n",
    "This tree shows a sequence of decisions leading to the classification outcome.\n",
    "\n",
    "---\n",
    "\n",
    "### **Advantages of Decision Trees**\n",
    "\n",
    "- Easy to **understand** and **visualize**.  \n",
    "- Handles both **numerical** and **categorical** data.  \n",
    "- Does not require feature scaling (normalization).  \n",
    "- Can capture **non-linear relationships**.\n",
    "\n",
    "\n",
    "\n",
    "### **Limitations**\n",
    "\n",
    "- **Overfitting:** May fit noise if the tree becomes too deep.  \n",
    "- **Unstable:** Small changes in data can change the tree structure.  \n",
    "- **Bias toward dominant features** if data is not balanced.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d92f303-58e0-4639-a170-c1592c12eddf",
   "metadata": {},
   "source": [
    "## Question 2: Explain the concepts of Gini Impurity and Entropy as impurity measures. How do they impact the splits in a Decision Tree?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e81d05-ac07-4b56-aa67-a2240e81cb3e",
   "metadata": {},
   "source": [
    "\n",
    "### **Introduction**\n",
    "\n",
    "In a **Decision Tree**, the goal at each step is to **split the data** in a way that results in the **purest possible subsets** — meaning each subset contains mostly data points from a single class.  \n",
    "\n",
    "To measure this **purity or impurity**, we use **impurity measures** such as **Gini Impurity** and **Entropy**.\n",
    "\n",
    "\n",
    "\n",
    "### **1. Gini Impurity**\n",
    "\n",
    "**Definition:**  \n",
    "Gini Impurity measures how often a randomly chosen data point from the dataset would be **incorrectly classified** if it were labeled according to the class distribution in that subset.\n",
    "\n",
    "**Formula:**\n",
    "\n",
    "\\[\n",
    "Gini = 1 - \\sum_{i=1}^{C} (p_i)^2\n",
    "\\]\n",
    "\n",
    "Where:  \n",
    "- \\( C \\) = Number of classes  \n",
    "- \\( p_i \\) = Probability of a data point belonging to class \\( i \\)\n",
    "\n",
    "**Interpretation:**\n",
    "- \\( Gini = 0 \\): The node is **pure** (all samples belong to one class).  \n",
    "- \\( Gini \\) increases as the classes become more mixed.  \n",
    "\n",
    "**Example:**\n",
    "If a node contains 80% Class A and 20% Class B:\n",
    "\\[\n",
    "Gini = 1 - (0.8^2 + 0.2^2) = 1 - (0.64 + 0.04) = 0.32\n",
    "\\]\n",
    "So, the impurity is **0.32**.\n",
    "\n",
    "\n",
    "\n",
    "### **2. Entropy**\n",
    "\n",
    "**Definition:**  \n",
    "Entropy measures the **amount of randomness or disorder** in the dataset. It originates from **information theory** and is used in decision trees built with the **Information Gain** criterion (like in ID3 algorithm).\n",
    "\n",
    "**Formula:**\n",
    "\n",
    "\\[\n",
    "Entropy = - \\sum_{i=1}^{C} p_i \\log_2(p_i)\n",
    "\\]\n",
    "\n",
    "Where:  \n",
    "- \\( p_i \\) = Probability of class \\( i \\)\n",
    "\n",
    "**Interpretation:**\n",
    "- \\( Entropy = 0 \\): Node is pure (only one class present).  \n",
    "- \\( Entropy \\) is **maximum** when classes are evenly distributed.\n",
    "\n",
    "**Example:**\n",
    "For 50% Class A and 50% Class B:\n",
    "\\[\n",
    "Entropy = - (0.5 \\log_2 0.5 + 0.5 \\log_2 0.5) = 1\n",
    "\\]\n",
    "So, the node has **maximum impurity**.\n",
    "\n",
    "\n",
    "\n",
    "### **3. How They Impact the Splits**\n",
    "\n",
    "During tree building:\n",
    "1. The algorithm calculates **Gini** or **Entropy** for all possible splits.\n",
    "2. It selects the split that produces the **largest reduction in impurity** — that is, the **most pure child nodes**.\n",
    "\n",
    "This reduction is known as **Information Gain**:\n",
    "\n",
    "\\[\n",
    "Information\\ Gain = Entropy_{parent} - \\sum_{j} \\frac{N_j}{N} Entropy_{child_j}\n",
    "\\]\n",
    "\n",
    "or, when using Gini:\n",
    "\\[\n",
    "Gini\\ Gain = Gini_{parent} - \\sum_{j} \\frac{N_j}{N} Gini_{child_j}\n",
    "\\]\n",
    "\n",
    "\n",
    "\n",
    "### **4. Comparison Between Gini and Entropy**\n",
    "\n",
    "| Criteria | Gini Impurity | Entropy |\n",
    "|-----------|----------------|----------|\n",
    "| Range | 0 to 0.5 (for binary classification) | 0 to 1 |\n",
    "| Computation | Simpler and faster | Slightly more complex |\n",
    "| Behavior | Tends to isolate the most frequent class | More sensitive to class imbalance |\n",
    "| Used In | CART Algorithm | ID3, C4.5 Algorithms |\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0fec95-e551-4dfa-a45d-c2011c7f8f72",
   "metadata": {},
   "source": [
    "\n",
    "### **Introduction**\n",
    "\n",
    "A **Decision Tree** algorithm recursively splits data into smaller subsets based on feature values to build a predictive model.  \n",
    "However, if the tree keeps splitting until every data point is perfectly classified, it may become **too complex** and start **overfitting** — meaning it performs well on training data but poorly on unseen data.\n",
    "\n",
    "To prevent overfitting and improve generalization, we use a process called **Pruning**.\n",
    "\n",
    "**Pruning** means **reducing the size of a decision tree** by removing unnecessary branches or nodes that do not contribute much to the model’s predictive power.\n",
    "\n",
    "There are **two main types** of pruning:\n",
    "1. **Pre-Pruning (Early Stopping)**\n",
    "2. **Post-Pruning (Pruning After Training)**\n",
    "\n",
    "\n",
    "\n",
    "### **1. Pre-Pruning (Early Stopping)**\n",
    "\n",
    "**Definition:**  \n",
    "Pre-pruning stops the tree-building process **early**, before it becomes too complex.  \n",
    "In this approach, the algorithm decides **not to split a node** if the split does not provide significant improvement in purity (measured using **Gini Impurity** or **Entropy**).\n",
    "\n",
    "\n",
    "\n",
    "#### **How It Works**\n",
    "\n",
    "During the training phase, the algorithm evaluates each possible split.  \n",
    "It checks whether the split improves the model enough to justify the increase in complexity.\n",
    "\n",
    "If the improvement (known as **information gain**) is below a certain **threshold**, the algorithm **stops splitting** further.\n",
    "\n",
    "\\[\n",
    "Information\\ Gain = Impurity_{parent} - \\sum_{j=1}^{k} \\frac{N_j}{N} \\times Impurity_{child_j}\n",
    "\\]\n",
    "\n",
    "If `Information Gain < Minimum Threshold`, the split is **not made**.\n",
    "\n",
    "\n",
    "\n",
    "#### **Common Pre-Pruning Parameters (in Scikit-learn)**\n",
    "\n",
    "- `max_depth` → maximum depth of the tree  \n",
    "- `min_samples_split` → minimum number of samples required to split a node  \n",
    "- `min_samples_leaf` → minimum samples allowed in a leaf node  \n",
    "- `max_leaf_nodes` → limits total number of leaf nodes  \n",
    "- `min_impurity_decrease` → minimum decrease in impurity needed for a split  \n",
    "\n",
    "---\n",
    "\n",
    "#### **Example**\n",
    "\n",
    "Suppose we are classifying emails as “Spam” or “Not Spam”.  \n",
    "If splitting on the word *“offer”* only improves accuracy by 0.1%, the algorithm might decide not to split further — avoiding overfitting to noise words.\n",
    "\n",
    "\n",
    "\n",
    "#### **Practical Advantage of Pre-Pruning**\n",
    "\n",
    " **Advantage:**  \n",
    "- **Saves computation time and prevents overfitting early.**  \n",
    "Since the tree stops growing when improvement becomes insignificant, it produces a **simpler, faster, and more generalizable** model.\n",
    "\n",
    "\n",
    "### **2. Post-Pruning (Pruning After Training)**\n",
    "\n",
    "**Definition:**  \n",
    "Post-pruning allows the decision tree to **grow fully** — potentially overfitting — and then **removes unimportant branches** afterward.  \n",
    "This process simplifies the model without losing much predictive accuracy.\n",
    "\n",
    "\n",
    "#### **How It Works**\n",
    "\n",
    "1. The algorithm first grows a **complete tree** using all training data.  \n",
    "2. It then evaluates the **importance of each branch or leaf** using a validation set or cross-validation.  \n",
    "3. Subtrees or branches that contribute **little to accuracy** are pruned (removed).  \n",
    "4. The pruning continues until the model’s performance stops improving.\n",
    "\n",
    "\n",
    "\n",
    "#### **Common Post-Pruning Methods**\n",
    "\n",
    "- **Reduced Error Pruning:**  \n",
    "  - Prunes nodes only if removing them does not reduce accuracy on the validation set.  \n",
    "- **Cost Complexity Pruning (CCP):**  \n",
    "  - Introduced in CART algorithm.  \n",
    "  - Adds a penalty for tree complexity using the formula:  \n",
    "\n",
    "\\[\n",
    "R_{\\alpha}(T) = R(T) + \\alpha |T|\n",
    "\\]\n",
    "\n",
    "Where:  \n",
    "- \\( R(T) \\): Total misclassification error of the tree  \n",
    "- \\( |T| \\): Number of leaf nodes (complexity)  \n",
    "- \\( \\alpha \\): Complexity parameter (controls pruning strength)\n",
    "\n",
    "By adjusting \\( \\alpha \\), the algorithm balances **accuracy vs. simplicity**.\n",
    "\n",
    "\n",
    "\n",
    "#### **Example**\n",
    "\n",
    "In a decision tree for predicting loan approvals, post-pruning may remove branches related to extremely rare cases (like income > ₹10 million), which add complexity but don’t improve prediction accuracy.\n",
    "\n",
    "\n",
    "#### **Practical Advantage of Post-Pruning**\n",
    "\n",
    " **Advantage:**  \n",
    "- **Produces a more accurate and generalizable model.**  \n",
    "Since pruning occurs **after** full growth, it ensures that no potentially useful patterns are missed during the initial training.\n",
    "\n",
    "\n",
    "\n",
    "### **3. Key Differences Between Pre-Pruning and Post-Pruning**\n",
    "\n",
    "| **Feature** | **Pre-Pruning (Early Stopping)** | **Post-Pruning (After Training)** |\n",
    "|--------------|----------------------------------|-----------------------------------|\n",
    "| **Timing** | Stops tree growth during training | Prunes the tree after it is fully grown |\n",
    "| **Goal** | Prevent overfitting early | Remove overfitted branches after learning |\n",
    "| **Computation** | Less computationally expensive | More computationally intensive |\n",
    "| **Risk** | Might stop too early and underfit | Allows full learning, but then simplifies |\n",
    "| **Example Parameters** | `max_depth`, `min_samples_split` | `ccp_alpha`, validation-based pruning |\n",
    "\n",
    "\n",
    "\n",
    "### **4. Summary**\n",
    "\n",
    "- **Pre-Pruning** limits tree growth using early stopping criteria → reduces overfitting and saves computation time.  \n",
    "- **Post-Pruning** grows a complete tree first, then removes unnecessary parts → improves model performance and generalization.  \n",
    "\n",
    "Both techniques aim to build a **simpler, more robust Decision Tree** that performs well on unseen data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c1f758-daaa-4d48-90ad-6eb1b26d388b",
   "metadata": {},
   "source": [
    "## Question 4: What is Information Gain in Decision Trees, and why is it important for choosing the best split?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aaf489c-5dfe-40ba-a4d6-83d74c545064",
   "metadata": {},
   "source": [
    "\n",
    "### **Introduction**\n",
    "\n",
    "In **Decision Tree algorithms**, the most important step is **deciding where to split the data** at each node.  \n",
    "The quality of each split determines how well the tree classifies the data.  \n",
    "\n",
    "To measure this quality, we use **Information Gain (IG)** — a metric that quantifies how much **“information” or “purity”** is gained after a split.  \n",
    "\n",
    "Information Gain is primarily used when the **Entropy** measure is chosen as the impurity criterion (as in **ID3** and **C4.5** algorithms).\n",
    "\n",
    "\n",
    "\n",
    "### **Definition**\n",
    "\n",
    "**Information Gain** measures the **reduction in entropy (impurity)** achieved by partitioning the dataset based on a particular feature.\n",
    "\n",
    "It tells us **how much uncertainty in the dataset decreases** when we split it on a given attribute.\n",
    "\n",
    "\n",
    "\n",
    "### **Formula for Information Gain**\n",
    "\n",
    "\\[\n",
    "Information\\ Gain (IG) = Entropy(Parent) - \\sum_{i=1}^{k} \\frac{N_i}{N} \\times Entropy(Child_i)\n",
    "\\]\n",
    "\n",
    "Where:  \n",
    "- \\( Entropy(Parent) \\) → Entropy of the original dataset (before splitting)  \n",
    "- \\( Entropy(Child_i) \\) → Entropy of each child subset after splitting  \n",
    "- \\( N_i \\) → Number of samples in the \\( i^{th} \\) child node  \n",
    "- \\( N \\) → Total number of samples in the parent node  \n",
    "\n",
    "\n",
    "\n",
    "### **Step-by-Step Explanation**\n",
    "\n",
    "1. **Compute Parent Entropy:**  \n",
    "   Calculate the entropy of the original dataset before any split.\n",
    "\n",
    "2. **Split the Data:**  \n",
    "   Divide the dataset into subsets based on one feature (e.g., “Age > 30?”).\n",
    "\n",
    "3. **Compute Child Entropies:**  \n",
    "   For each subset, compute the entropy again.\n",
    "\n",
    "4. **Compute Weighted Average of Child Entropies:**  \n",
    "   Weight each child’s entropy by the proportion of samples it contains.\n",
    "\n",
    "5. **Calculate Information Gain:**  \n",
    "   Subtract the weighted child entropy from the parent entropy.\n",
    "\n",
    "\n",
    "### **Interpretation**\n",
    "\n",
    "- A **higher Information Gain** means the feature provides **more useful information** for classifying data.  \n",
    "- The **feature with the highest Information Gain** is selected as the **best split** at that node.\n",
    "\n",
    "\n",
    "\n",
    "### **Example**\n",
    "\n",
    "Suppose we are predicting whether a student **passes** or **fails** based on whether they **study regularly**.\n",
    "\n",
    "| Student | Studies Regularly | Result |\n",
    "|----------|-------------------|--------|\n",
    "| A | Yes | Pass |\n",
    "| B | Yes | Pass |\n",
    "| C | No | Fail |\n",
    "| D | No | Fail |\n",
    "| E | Yes | Pass |\n",
    "\n",
    "#### **Step 1: Calculate Parent Entropy**\n",
    "\n",
    "Total = 5 students → 3 Pass, 2 Fail  \n",
    "\\[\n",
    "Entropy(Parent) = - \\left( \\frac{3}{5} \\log_2 \\frac{3}{5} + \\frac{2}{5} \\log_2 \\frac{2}{5} \\right) = 0.971\n",
    "\\]\n",
    "\n",
    "#### **Step 2: Split on “Studies Regularly”**\n",
    "\n",
    "- **Yes group:** (3 Pass, 0 Fail) → Entropy = 0  \n",
    "- **No group:** (0 Pass, 2 Fail) → Entropy = 0  \n",
    "\n",
    "#### **Step 3: Weighted Entropy After Split**\n",
    "\n",
    "\\[\n",
    "Entropy(Children) = \\frac{3}{5}(0) + \\frac{2}{5}(0) = 0\n",
    "\\]\n",
    "\n",
    "#### **Step 4: Information Gain**\n",
    "\n",
    "\\[\n",
    "IG = 0.971 - 0 = 0.971\n",
    "\\]\n",
    "\n",
    "**High Information Gain (0.971)** means “Studies Regularly” is an excellent attribute to split on — it perfectly classifies the data.\n",
    "\n",
    "\n",
    "\n",
    "### **Why Information Gain is Important**\n",
    "\n",
    "1. **Feature Selection:**  \n",
    "   Information Gain helps the Decision Tree **choose the most informative feature** at each node.\n",
    "\n",
    "2. **Efficient Splitting:**  \n",
    "   It ensures that every split **maximizes class purity**, leading to faster and more accurate learning.\n",
    "\n",
    "3. **Reduces Uncertainty:**  \n",
    "   Each split reduces the randomness (entropy) of the dataset, making predictions more certain.\n",
    "\n",
    "4. **Improves Model Accuracy:**  \n",
    "   Higher Information Gain leads to better generalization and less misclassification.\n",
    "\n",
    "\n",
    "\n",
    "### **Relation to Entropy**\n",
    "\n",
    "Information Gain is directly based on **Entropy**, which measures impurity:\n",
    "\n",
    "\\[\n",
    "Entropy = - \\sum_{i=1}^{C} p_i \\log_2(p_i)\n",
    "\\]\n",
    "\n",
    "- High entropy → High disorder (mixed classes).  \n",
    "- Low entropy → High purity (mostly one class).  \n",
    "\n",
    "So, a split that **reduces entropy the most** will have the **highest Information Gain**.\n",
    "\n",
    "\n",
    "\n",
    "### **Advantages of Using Information Gain**\n",
    "\n",
    "- Provides a **quantitative measure** for evaluating splits.  \n",
    "- Works well with **categorical attributes**.  \n",
    "- Encourages **pure and interpretable** tree structures.\n",
    "\n",
    "\n",
    "\n",
    "### **Limitations**\n",
    "\n",
    "- Tends to favor features with **many unique values** (e.g., ID numbers).  \n",
    "  → This is addressed by using **Gain Ratio** (used in C4.5 algorithm).  \n",
    "- Computationally more expensive than Gini Impurity.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9b6445-b70c-4a62-bdb4-e8d9ae74b0e1",
   "metadata": {},
   "source": [
    "\n",
    "### **Introduction**\n",
    "\n",
    "A **Decision Tree** is a supervised machine learning algorithm used for both **classification** and **regression** problems.  \n",
    "It operates by recursively splitting the dataset into smaller and smaller subsets based on the most significant attributes (features), forming a tree-like structure of decisions and outcomes.  \n",
    "Each internal node represents a **test on an attribute**, each branch represents the **result of that test**, and each leaf node represents a **final decision or output**.\n",
    "\n",
    "Decision Trees are popular because they are **easy to interpret**, **require little data preprocessing**, and **mimic human decision-making**.  \n",
    "They are frequently used in various real-world applications due to their transparency and flexibility.\n",
    "\n",
    "\n",
    "\n",
    "### **1. Real-World Applications of Decision Trees**\n",
    "\n",
    "\n",
    "\n",
    "#### **1.1 Iris Dataset – Classification Task**\n",
    "\n",
    "One of the most common applications of Decision Trees is **classification**.  \n",
    "The **Iris dataset** is a classic example used to classify flowers into three species based on their physical features.\n",
    "\n",
    "- **Dataset Information:**\n",
    "  - Features: Sepal Length, Sepal Width, Petal Length, Petal Width\n",
    "  - Target: Species of Iris (Setosa, Versicolor, Virginica)\n",
    "\n",
    "- **Working Example:**\n",
    "  A Decision Tree might split data as follows:\n",
    "If (Petal Length < 2.5 cm) → Iris-setosa\n",
    "Else if (Petal Width < 1.8 cm) → Iris-versicolor\n",
    "Else → Iris-virginica\n",
    "\n",
    "\n",
    "- **Practical Use Case:**  \n",
    "Such classification systems can be adapted for **automated plant identification**, **image-based species recognition**, or **biological research** applications.\n",
    "\n",
    "---\n",
    "\n",
    "#### **1.2 Boston Housing Dataset – Regression Task**\n",
    "\n",
    "Decision Trees can also perform **regression**, predicting continuous values instead of categories.  \n",
    "The **Boston Housing dataset** is widely used for this purpose.\n",
    "\n",
    "- **Dataset Information:**\n",
    "- Features: Average number of rooms, crime rate, accessibility to highways, property tax rate, etc.\n",
    "- Target: Median value of owner-occupied homes in $1000s.\n",
    "\n",
    "- **Working Example:**\n",
    "A Decision Tree might split as:\n",
    "If (RM > 6.5) and (LSTAT < 10%) → House Price ≈ High\n",
    "Else if (RM < 5.5) → House Price ≈ Low\n",
    "Else → House Price ≈ Medium\n",
    "\n",
    "\n",
    "- **Practical Use Case:**  \n",
    "This can help **real estate companies** or **urban planners** predict housing prices and plan infrastructure development.\n",
    "\n",
    "---\n",
    "\n",
    "#### **1.3 Additional Real-World Examples**\n",
    "\n",
    "- **Finance:** Credit risk analysis, loan approval, and fraud detection.  \n",
    "- **Healthcare:** Predicting diseases based on symptoms and medical test results.  \n",
    "- **Marketing:** Customer segmentation, churn prediction, and sales forecasting.  \n",
    "- **Manufacturing:** Quality control and predictive maintenance of machinery.  \n",
    "- **Education:** Student performance prediction and adaptive learning systems.\n",
    "\n",
    "\n",
    "\n",
    "### **2. Advantages of Decision Trees**\n",
    "\n",
    "1. **Easy to Understand and Interpret:**  \n",
    " The tree structure is visual and similar to human reasoning, making it easy for non-technical users to interpret.\n",
    "\n",
    "2. **Handles Both Numerical and Categorical Data:**  \n",
    " Decision Trees can work with various data types without requiring feature scaling or normalization.\n",
    "\n",
    "3. **Requires Minimal Data Preparation:**  \n",
    " Unlike many algorithms, trees don’t require feature standardization or dummy encoding.\n",
    "\n",
    "4. **Useful for Feature Selection:**  \n",
    " Decision Trees automatically rank features by importance during training, helping identify key predictors.\n",
    "\n",
    "5. **Works Well on Nonlinear Relationships:**  \n",
    " Can model complex relationships between features without assuming linearity.\n",
    "\n",
    "\n",
    "\n",
    "### **3. Limitations of Decision Trees**\n",
    "\n",
    "1. **Overfitting:**  \n",
    " Trees can become overly complex and fit noise in the training data, reducing generalization on unseen data.\n",
    "\n",
    "2. **Instability:**  \n",
    " Small changes in data can drastically change the structure of the tree.\n",
    "\n",
    "3. **Biased Toward Features with More Levels:**  \n",
    " Features with more categories may dominate the splitting process.\n",
    "\n",
    "4. **Less Accurate Alone:**  \n",
    " While easy to interpret, single trees may not achieve the same accuracy as ensemble models like **Random Forests** or **Gradient Boosted Trees**.\n",
    "\n",
    "5. **Computational Cost:**  \n",
    " For large datasets, training and pruning can become computationally expensive.\n",
    "\n",
    "\n",
    "\n",
    "### **4. Summary of Datasets**\n",
    "\n",
    "| Dataset | Task Type | Example Use | scikit-learn Loader |\n",
    "|----------|------------|--------------|---------------------|\n",
    "| **Iris Dataset** | Classification | Predicting flower species | `sklearn.datasets.load_iris()` |\n",
    "| **Boston Housing Dataset** | Regression | Predicting house prices | `sklearn.datasets.load_boston()` *(or via CSV)* |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf235d52-79b3-440d-89f3-9dc1298f9d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Tree Depth: 5\n",
      "Regression Tree Depth: 40\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris, fetch_california_housing\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "\n",
    "# Classification using Iris dataset\n",
    "iris = load_iris()\n",
    "clf = DecisionTreeClassifier(random_state=42)\n",
    "clf.fit(iris.data, iris.target)\n",
    "\n",
    "# Regression using California Housing dataset (modern replacement for Boston)\n",
    "housing = fetch_california_housing()\n",
    "reg = DecisionTreeRegressor(random_state=42)\n",
    "reg.fit(housing.data, housing.target)\n",
    "\n",
    "print(\"Classification Tree Depth:\", clf.get_depth())\n",
    "print(\"Regression Tree Depth:\", reg.get_depth())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a88fe97-976b-4379-9c86-c39d80345f02",
   "metadata": {},
   "source": [
    "## Question 6: Write a Python program to:\n",
    "## ● Load the Iris Dataset\n",
    "## ● Train a Decision Tree Classifier using the Gini criterion\n",
    "## ● Print the model’s accuracy and feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "369e5e3c-db68-4dac-840d-3c3e0669245a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 100.0 %\n",
      "\n",
      "Feature Importances:\n",
      "sepal length (cm): 0.0\n",
      "sepal width (cm): 0.0191\n",
      "petal length (cm): 0.8933\n",
      "petal width (cm): 0.0876\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "\n",
    "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Model Accuracy:\", round(accuracy * 100, 2), \"%\")\n",
    "\n",
    "\n",
    "print(\"\\nFeature Importances:\")\n",
    "for feature, importance in zip(iris.feature_names, clf.feature_importances_):\n",
    "    print(f\"{feature}: {round(importance, 4)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db67aa5-54a2-4356-bb1a-b05613590b60",
   "metadata": {},
   "source": [
    "## Question 7: Write a Python program to:\n",
    "## ● Load the Iris Dataset\n",
    "## ● Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to a fully-grown tree.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94dab69b-9021-41e7-9a5a-a4403fa53497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree with max_depth=3 Accuracy:  100.0 %\n",
      "Fully-grown Decision Tree Accuracy:  100.0 %\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "clf_limited = DecisionTreeClassifier(criterion='gini', max_depth=3, random_state=42)\n",
    "clf_limited.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "clf_full = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
    "clf_full.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred_limited = clf_limited.predict(X_test)\n",
    "y_pred_full = clf_full.predict(X_test)\n",
    "\n",
    "\n",
    "acc_limited = accuracy_score(y_test, y_pred_limited)\n",
    "acc_full = accuracy_score(y_test, y_pred_full)\n",
    "\n",
    "\n",
    "print(\"Decision Tree with max_depth=3 Accuracy: \", round(acc_limited * 100, 2), \"%\")\n",
    "print(\"Fully-grown Decision Tree Accuracy: \", round(acc_full * 100, 2), \"%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e5aec4-5eda-451e-884d-1a1208490b6c",
   "metadata": {},
   "source": [
    "## Question 8: Write a Python program to:\n",
    "## ● Load the Boston Housing Dataset\n",
    "## ● Train a Decision Tree Regressor\n",
    "## ● Print the Mean Squared Error (MSE) and feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98848296-2bd6-4b9e-9fa4-0f0650d995a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE): 0.53\n",
      "\n",
      "Feature Importances:\n",
      "MedInc: 0.5235\n",
      "HouseAge: 0.0521\n",
      "AveRooms: 0.0494\n",
      "AveBedrms: 0.025\n",
      "Population: 0.0322\n",
      "AveOccup: 0.139\n",
      "Latitude: 0.09\n",
      "Longitude: 0.0888\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "X = housing.data\n",
    "y = housing.target\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "regressor = DecisionTreeRegressor(random_state=42)\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred = regressor.predict(X_test)\n",
    "\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "\n",
    "print(\"Mean Squared Error (MSE):\", round(mse, 2))\n",
    "\n",
    "\n",
    "print(\"\\nFeature Importances:\")\n",
    "for name, importance in zip(housing.feature_names, regressor.feature_importances_):\n",
    "    print(f\"{name}: {round(importance, 4)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6764390b-376e-4f56-b4cb-d1c3123cb1c4",
   "metadata": {},
   "source": [
    "## Question 9: Write a Python program to:\n",
    "## ● Load the Iris Dataset\n",
    "## ● Tune the Decision Tree’s max_depth and min_samples_split using GridSearchCV\n",
    "## ● Print the best parameters and the resulting model accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "186f6785-7608-418d-9023-d7cdb8c1eae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters Found: {'max_depth': 4, 'min_samples_split': 6}\n",
      "Best Cross-Validation Accuracy: 94.29 %\n",
      "Test Set Accuracy with Best Model: 100.0 %\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'max_depth': [2, 3, 4, 5, 6, None],\n",
    "    'min_samples_split': [2, 3, 4, 5, 6]\n",
    "}\n",
    "\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=dt,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,             \n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1         \n",
    ")\n",
    "\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "y\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "print(\"Best Parameters Found:\", grid_search.best_params_)\n",
    "print(\"Best Cross-Validation Accuracy:\", round(grid_search.best_score_ * 100, 2), \"%\")\n",
    "print(\"Test Set Accuracy with Best Model:\", round(accuracy * 100, 2), \"%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbbbdd9-10db-4c9d-97c1-21738b3f2cdd",
   "metadata": {},
   "source": [
    "## Question 10: Imagine you’re working as a data scientist for a healthcare company thatwants to predict whether a patient has a certain disease. You have a large dataset with mixed data types and some missing values.\n",
    "## Explain the step-by-step process you would follow to:\n",
    "## ● Handle the missing values\n",
    "## ● Encode the categorical features\n",
    "## ● Train a Decision Tree model\n",
    "## ● Tune its hyperparameters\n",
    "## ● Evaluate its performance And describe what business value this model could provide in the real-world setting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f811fc-1d53-481c-8ffe-871376e7a6f0",
   "metadata": {},
   "source": [
    "### Problem context\n",
    "You have a large healthcare dataset (mixed numerical + categorical features, some missing values). Goal: predict whether a patient has a disease (binary classification).\n",
    "\n",
    "\n",
    "\n",
    "## 1. Understand the data (first, before modeling)\n",
    "**Why:** good decisions about imputation, encoding and evaluation depend on data characteristics.\n",
    "\n",
    "**Actions**\n",
    "- Inspect column types, missingness pattern, cardinality of categorical features, class balance (positive vs negative cases), distributions and outliers.\n",
    "- Ask whether missingness is likely MCAR / MAR / MNAR (domain knowledge). For example, a missing lab test might mean “test not ordered” (informative).\n",
    "- Calculate baseline metrics (e.g., base prevalence) so you know what “good” looks like.\n",
    "\n",
    "**Checks**\n",
    "- `df.isnull().mean()` per column (missing rate).\n",
    "- `df.nunique()` for categorical cardinalities.\n",
    "- Class counts: `y.value_counts(normalize=True)`.\n",
    "\n",
    "\n",
    "\n",
    "## 2. Handling missing values\n",
    "**Principles**\n",
    "- Use domain knowledge. Missingness can be informative (create indicator flag).\n",
    "- Avoid data leakage: impute using only training data statistics inside a pipeline.\n",
    "\n",
    "**Strategies (by feature type & missing rate)**\n",
    "\n",
    "1. **Low missingness (e.g., <5–10%)**\n",
    "   - Numeric: median (robust) or mean if normal.\n",
    "   - Categorical: constant like `'missing'` or the mode.\n",
    "\n",
    "2. **Moderate missingness**\n",
    "   - Numeric: median or model-based imputation (KNN, Iterative Imputer / MICE).\n",
    "   - Categorical: impute with `'missing'` or create a new category; consider target/impact encoding (careful with leakage).\n",
    "\n",
    "3. **High missingness (>30–50%)**\n",
    "   - Consider dropping the feature unless domain knowledge says it’s essential.\n",
    "   - Or engineer a binary indicator `feature_missing` and treat the feature carefully.\n",
    "\n",
    "4. **Informative missingness**\n",
    "   - Create a missing indicator column (`feature_X_missing`) and use it as a predictor.\n",
    "\n",
    "5. **Time-dependent / longitudinal labs**\n",
    "   - If multiple measurements exist, extract summary statistics (last, max, trend) rather than naively imputing.\n",
    "\n",
    "**Recommended tools (scikit-learn)**\n",
    "- `sklearn.impute.SimpleImputer` (mean/median/most_frequent/constant)\n",
    "- `sklearn.impute.KNNImputer` (if reasonable size)\n",
    "- `sklearn.impute.IterativeImputer` (MICE-like), but ensure performance & stability\n",
    "\n",
    "**Important:** Put imputation **inside** a pipeline and fit only on training folds to avoid leakage.\n",
    "\n",
    "\n",
    "## 3. Encoding categorical features\n",
    "**Principles**\n",
    "- Use encoders that match feature cardinality and model type.\n",
    "- One-hot for low-cardinality categories.\n",
    "- For high-cardinality categorical features consider target encoding / count encoding / embeddings — but be careful to avoid target leakage (use nested CV or smoothing).\n",
    "\n",
    "**Options**\n",
    "- **One-Hot Encoding** (`OneHotEncoder` with `handle_unknown='ignore'`): works well for small cardinalities and tree models tolerate sparse one-hots.\n",
    "- **Ordinal Encoding** (`OrdinalEncoder`): only if categories have natural order.\n",
    "- **Count / Frequency Encoding:** replace category by its frequency — good for high cardinality.\n",
    "- **Target Encoding / Mean Encoding:** can improve performance for high cardinality but MUST be done with cross-validation or using smoothing to prevent leakage.\n",
    "- **Leave-one-out / CatBoost-style encoders**: alternatives that reduce leakage risk.\n",
    "\n",
    "**Practical rule of thumb**\n",
    "- If cardinality <= 10: One-hot.\n",
    "- If cardinality > 30: use target/count encoding or hashing (with careful CV).\n",
    "\n",
    "\n",
    "\n",
    "## 4. Feature engineering and scaling\n",
    "**Decision Trees do not require scaling**, but:\n",
    "- Create interaction / domain features (e.g., BMI from height & weight).\n",
    "- Aggregate lab time series (last value, slope).\n",
    "- Binarize clinically meaningful thresholds (e.g., `age >= 65`).\n",
    "- Add missingness indicators.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Train a Decision Tree (proper pipeline & cross-validation)\n",
    "**Why pipeline?** ensures preprocessing + imputation + encoding are applied identically in CV and deployment, and prevents leakage.\n",
    "\n",
    "**Suggested pipeline (sklearn)**\n",
    "- `ColumnTransformer` to apply different imputers/encoders to numeric vs categorical.\n",
    "- `Pipeline` with Transformer → `DecisionTreeClassifier`.\n",
    "\n",
    "**Use stratified splits** because disease prevalence may be low (stratify by label).\n",
    "\n",
    "**Handle class imbalance**\n",
    "- Use `class_weight='balanced'` or supply `sample_weight`.\n",
    "- Also consider resampling (SMOTE, undersampling) but do that inside CV pipeline to avoid leakage.\n",
    "\n",
    "\n",
    "\n",
    "## 6. Hyperparameter tuning\n",
    "**Key DecisionTree hyperparameters**\n",
    "- `max_depth` (controls overfitting)\n",
    "- `min_samples_split`\n",
    "- `min_samples_leaf`\n",
    "- `max_features` (subset of features to consider at each split)\n",
    "- `ccp_alpha` (cost complexity pruning)\n",
    "- `criterion` (`gini` or `entropy`) — usually minor differences\n",
    "\n",
    "**Tuning approach**\n",
    "- Use `GridSearchCV` or `RandomizedSearchCV` with **StratifiedKFold** and scoring tuned to business objective (e.g., `'roc_auc'`, `'average_precision'`, or custom scoring that weights false negatives more).\n",
    "- Use nested CV if you need an unbiased estimate of generalization performance when you also report tuned CV performance.\n",
    "- Use `n_jobs=-1` to parallelize.\n",
    "\n",
    "**Example parameter grid**\n",
    "```py\n",
    "param_grid = {\n",
    "  'clf__max_depth': [3, 5, 7, 10, None],\n",
    "  'clf__min_samples_split': [2, 5, 10, 20],\n",
    "  'clf__min_samples_leaf': [1, 2, 5, 10],\n",
    "  'clf__ccp_alpha': [0.0, 0.001, 0.01, 0.1]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebfff8f-13fe-43cf-843b-790f74bbb44e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
