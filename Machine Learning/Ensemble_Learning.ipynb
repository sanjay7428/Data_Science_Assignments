{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "984270e0-8d49-4388-b895-adc299707d6b",
   "metadata": {},
   "source": [
    "## Ensemble Learning | Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6684f3a6-9c5b-43fb-9a8a-578f6c0c2c95",
   "metadata": {},
   "source": [
    "## Question 1: What is Ensemble Learning in machine learning? Explain the key idea behind it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71de2461-7eb3-416b-bf15-da9913c93c23",
   "metadata": {},
   "source": [
    "  \n",
    "### Explain the key idea behind it\n",
    "\n",
    "**Ensemble Learning** is an advanced technique in **machine learning** that combines multiple individual models, often referred to as **weak learners**, to build a **stronger and more accurate predictive model**. The main principle behind ensemble learning is that instead of relying on a single model, we can achieve better performance by aggregating the knowledge and predictions of several models. Each individual model may make some errors, but by combining them, these errors tend to cancel each other out, leading to a more accurate and reliable final prediction.\n",
    "\n",
    "In the real world, data can be noisy, complex, and difficult to model perfectly using a single learning algorithm. Therefore, ensemble learning provides a way to **increase generalization**, **reduce variance**, and **avoid overfitting**. This is achieved by training multiple models on variations of the dataset and then integrating their outputs in a systematic manner. The ensemble thus acts as a group of experts, each contributing its opinion toward the final decision.\n",
    "\n",
    "---\n",
    "\n",
    "##  Key Idea\n",
    "\n",
    "> ‚ÄúThe main idea of ensemble learning is that a group of weak models, when combined properly, can perform better than any individual strong model.‚Äù\n",
    "\n",
    "In simple terms, ensemble learning is similar to **taking the collective opinion of multiple experts** instead of trusting just one. For example, imagine asking several doctors to diagnose a patient ‚Äî while each doctor might give a slightly different opinion, combining their opinions usually leads to a more accurate diagnosis. Similarly, in machine learning, combining predictions from different models often results in improved accuracy and robustness.\n",
    "\n",
    "---\n",
    "\n",
    "##  How Ensemble Learning Works\n",
    "\n",
    "The process of ensemble learning generally involves three main steps:\n",
    "\n",
    "1. **Model Generation:**  \n",
    "   Multiple models are created using the same training data or variations of it. These models can be of the same type (for example, several decision trees) or different types (for example, a combination of decision tree, logistic regression, and SVM).\n",
    "\n",
    "2. **Model Combination:**  \n",
    "   The predictions from all the individual models are combined using a specific method such as **voting**, **averaging**, or **stacking**. The way models are combined depends on the type of ensemble method used.\n",
    "\n",
    "3. **Final Prediction:**  \n",
    "   The combined result is taken as the final prediction, which usually performs better than any individual model‚Äôs prediction.\n",
    "\n",
    "---\n",
    "\n",
    "## Example\n",
    "\n",
    "Suppose we are trying to predict whether an email is *spam* or *not spam*.  \n",
    "We can train three models:  \n",
    "- **Model 1:** Decision Tree  \n",
    "- **Model 2:** Logistic Regression  \n",
    "- **Model 3:** Support Vector Machine  \n",
    "\n",
    "Each model gives its own prediction. If two models predict *spam* and one predicts *not spam*, the ensemble method (through **majority voting**) will classify the email as *spam*.  \n",
    "This approach reduces the risk of relying on one model that may have made an incorrect prediction.\n",
    "\n",
    "---\n",
    "\n",
    "##  Types of Ensemble Methods\n",
    "\n",
    "There are three major types of ensemble learning techniques commonly used in machine learning:\n",
    "\n",
    "### 1. **Bagging (Bootstrap Aggregating)**\n",
    "Bagging is a method used to reduce **variance** and prevent **overfitting**. It involves training multiple models independently on different random subsets of the original dataset (sampled with replacement).  \n",
    "Each model gives a prediction, and the results are combined (usually by averaging or majority voting).  \n",
    "A popular example of bagging is the **Random Forest algorithm**, which builds multiple decision trees and combines their predictions.\n",
    "\n",
    "**Advantages of Bagging:**\n",
    "- Reduces variance  \n",
    "- Handles overfitting effectively  \n",
    "- Works well with unstable models like Decision Trees\n",
    "\n",
    "**Example:**\n",
    "```text\n",
    "Random Forest uses bagging of Decision Trees to achieve higher accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998fa654-b5db-43dd-879c-5637fd0a26a7",
   "metadata": {},
   "source": [
    "## Question 2: What is the difference between Bagging and Boosting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7511e27-b9bf-495e-9a02-6674c82e710d",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Bagging** and **Boosting** are two popular **ensemble learning techniques** in machine learning that aim to improve the performance, accuracy, and robustness of predictive models.  \n",
    "Although both combine multiple weak learners to create a strong learner, they differ in the way models are trained and combined.\n",
    "\n",
    "\n",
    "\n",
    "##  **1. Bagging (Bootstrap Aggregating)**\n",
    "\n",
    "**Bagging** is an ensemble technique that aims to reduce **variance** and prevent **overfitting**.  \n",
    "It works by training multiple models (usually of the same type, like Decision Trees) on different random subsets of the original dataset, created using **bootstrapping** (sampling with replacement). Each model is trained independently, and their outputs are combined (for example, through majority voting for classification or averaging for regression).\n",
    "\n",
    "### **Key Characteristics of Bagging:**\n",
    "- Models are trained **in parallel**, not dependent on each other.  \n",
    "- Each model gets a different subset of the training data.  \n",
    "- The final prediction is usually the **average (for regression)** or **majority vote (for classification)** of all models.  \n",
    "- Helps to reduce **variance** and overfitting, especially for unstable models.\n",
    "\n",
    "### **Example:**\n",
    "- **Random Forest** is the most popular Bagging-based algorithm. It builds many decision trees and combines their results to make a final prediction.\n",
    "\n",
    "\n",
    "##  **2. Boosting**\n",
    "\n",
    "**Boosting** is an ensemble method that aims to reduce **bias** and improve **accuracy** by building models **sequentially**.  \n",
    "Each new model is trained to correct the mistakes made by the previous ones. The algorithm gives **more weight** to misclassified or incorrectly predicted samples so that the next model can focus more on those difficult examples.\n",
    "\n",
    "### **Key Characteristics of Boosting:**\n",
    "- Models are trained **sequentially**, one after another.  \n",
    "- Each model depends on the performance of the previous models.  \n",
    "- Misclassified samples get **higher importance (weights)** in the next iteration.  \n",
    "- Helps to reduce **bias** and improve prediction accuracy.  \n",
    "- The final model is a **weighted combination** of all weak models.\n",
    "\n",
    "### **Examples of Boosting Algorithms:**\n",
    "- **AdaBoost (Adaptive Boosting)**  \n",
    "- **Gradient Boosting**  \n",
    "- **XGBoost**  \n",
    "- **LightGBM**  \n",
    "- **CatBoost**\n",
    "\n",
    "---\n",
    "\n",
    "##  **Key Differences Between Bagging and Boosting**\n",
    "\n",
    "| **Basis of Difference** | **Bagging** | **Boosting** |\n",
    "|--------------------------|-------------|---------------|\n",
    "| **Full Form** | Bootstrap Aggregating | ‚Äì |\n",
    "| **Main Objective** | Reduces variance and prevents overfitting | Reduces bias and improves accuracy |\n",
    "| **Training Process** | Models are trained **independently and in parallel** | Models are trained **sequentially**, one after another |\n",
    "| **Data Sampling** | Uses **random sampling with replacement** (bootstrapping) | Uses the **entire dataset**, but adjusts weights of samples |\n",
    "| **Model Focus** | All models are treated equally | Later models focus more on previously misclassified data |\n",
    "| **Error Correction** | No error correction from previous models | Each model tries to correct errors of the previous one |\n",
    "| **Combination Method** | Uses **averaging** (regression) or **majority voting** (classification) | Uses a **weighted average** based on model performance |\n",
    "| **Overfitting Tendency** | Reduces overfitting effectively | Can overfit if too many weak learners are added |\n",
    "| **Bias and Variance** | Reduces **variance** | Reduces **bias** |\n",
    "| **Examples** | Random Forest, Bagged Decision Trees | AdaBoost, Gradient Boosting, XGBoost |\n",
    "\n",
    "\n",
    "\n",
    "##  **Example Analogy**\n",
    "\n",
    "- **Bagging:** Think of several students solving the same exam independently. The teacher then combines their answers by taking a majority vote. Each student works independently ‚Äî this reduces random errors (variance).  \n",
    "- **Boosting:** Here, each student learns from the mistakes of the previous one. The next student focuses more on the questions the first student got wrong ‚Äî this process gradually reduces overall mistakes (bias).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3598cfa-d9bc-4237-9eaa-74247b22b0a3",
   "metadata": {},
   "source": [
    "## Question 3: What is bootstrap sampling and what role does it play in Bagging methods like Random Forest?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bccab01-b7aa-4ad5-8008-68fec6a858c0",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "##  **Definition of Bootstrap Sampling**\n",
    "\n",
    "**Bootstrap Sampling** is a statistical technique used to create multiple new datasets from an original dataset by **randomly sampling with replacement**.  \n",
    "This means that each time a data point is selected, it is returned to the dataset before the next selection ‚Äî allowing some data points to appear multiple times in the new sample, while others may not appear at all.\n",
    "\n",
    "The size of each bootstrap sample is usually the **same as the original dataset**, but the composition of samples differs because of the random selection process.  \n",
    "This approach is useful for estimating the accuracy and stability of models and is a core concept in **Bagging (Bootstrap Aggregating)** techniques.\n",
    "\n",
    "\n",
    "\n",
    "##  **How Bootstrap Sampling Works**\n",
    "\n",
    "1. Suppose we have an original dataset with **N data points**.  \n",
    "2. To create a bootstrap sample, we randomly select **N samples with replacement** from this dataset.  \n",
    "3. As sampling is done with replacement, some data points may be selected more than once, while some may not be selected at all.  \n",
    "4. This process is repeated multiple times to create **different bootstrap samples**.  \n",
    "5. A separate model (for example, a Decision Tree) is trained on each bootstrap sample.\n",
    "\n",
    "This results in several slightly different models, each trained on a different subset of the data.\n",
    "\n",
    "\n",
    "\n",
    "##  **Role of Bootstrap Sampling in Bagging and Random Forest**\n",
    "\n",
    "In **Bagging** methods like **Random Forest**, bootstrap sampling is used to train multiple models on different subsets of the data.  \n",
    "Each model (or tree, in the case of Random Forest) is trained on a different bootstrap sample, making each model unique and diverse. This diversity is crucial for the ensemble to perform better than individual models.\n",
    "\n",
    "### **Key Roles of Bootstrap Sampling in Bagging:**\n",
    "\n",
    "1. **Introduces Diversity Among Models:**  \n",
    "   Since each model is trained on a different bootstrap sample, they learn slightly different patterns from the data. This diversity helps the ensemble make better generalizations.\n",
    "\n",
    "2. **Reduces Overfitting:**  \n",
    "   Training on different samples prevents all models from fitting the same noise in the data, which reduces overfitting.\n",
    "\n",
    "3. **Improves Stability and Accuracy:**  \n",
    "   By combining predictions from multiple models (each trained on different samples), the overall ensemble prediction becomes more stable and accurate.\n",
    "\n",
    "4. **Supports Out-of-Bag (OOB) Error Estimation:**  \n",
    "   In Random Forests, the data points that are *not included* in a bootstrap sample are called **Out-of-Bag samples**.  \n",
    "   These samples are used to estimate model performance without the need for a separate validation dataset.\n",
    "\n",
    "\n",
    "\n",
    "##  **Example**\n",
    "\n",
    "Suppose we have a dataset with 5 records:  \n",
    "`[A, B, C, D, E]`\n",
    "\n",
    "A bootstrap sample (size 5) might look like:\n",
    "`[A, B, B, D, E]`  \n",
    "Another sample might be:\n",
    "`[A, A, C, D, D]`\n",
    "\n",
    "Each sample is used to train a separate model.  \n",
    "When we combine the predictions from all models (by averaging or voting), we get a more robust and accurate final prediction.\n",
    "\n",
    "---\n",
    "\n",
    "##  **In Random Forest:**\n",
    "- Multiple decision trees are trained using **different bootstrap samples**.  \n",
    "- Each tree is exposed to slightly different data, resulting in varied decision boundaries.  \n",
    "- The predictions from all trees are combined through **majority voting (for classification)** or **averaging (for regression)**.  \n",
    "- This process leads to improved model performance, reduced variance, and higher prediction accuracy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9a0754-05df-4d68-ba20-18803176418e",
   "metadata": {},
   "source": [
    "## Question 4: What are Out-of-Bag (OOB) samples and how is OOB score used to evaluate ensemble models?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fedee732-b3e9-4acb-b059-cdaa2e3cf4ef",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "##  **Definition of Out-of-Bag (OOB) Samples**\n",
    "\n",
    "In **Bagging (Bootstrap Aggregating)** methods such as **Random Forest**, each model (for example, each Decision Tree) is trained on a different **bootstrap sample** that is created by randomly selecting data points **with replacement** from the original dataset.\n",
    "\n",
    "Because sampling is done **with replacement**, some data points are selected multiple times while others are **not selected at all** in a particular bootstrap sample.  \n",
    "The data points **not included** in a specific bootstrap sample are called **Out-of-Bag (OOB) samples**.\n",
    "\n",
    "In general, for a dataset of size **N**, each bootstrap sample also has size **N**, but due to random selection with replacement, approximately **63% of the original data points** are included in the bootstrap sample, and the remaining **37%** are OOB samples.\n",
    "\n",
    "\n",
    "\n",
    "##  **Understanding OOB Samples with an Example**\n",
    "\n",
    "Let‚Äôs say we have a dataset with 5 records:  \n",
    "`[A, B, C, D, E]`\n",
    "\n",
    "Now we create a bootstrap sample (with replacement):  \n",
    "`[A, B, B, D, E]`\n",
    "\n",
    "Here, record **C** is not selected in this sample ‚Äî hence, **C** is an **Out-of-Bag sample** for this particular model.  \n",
    "Each model will have its own set of OOB samples depending on which records were selected during bootstrapping.\n",
    "\n",
    "\n",
    "##  **Role of OOB Samples in Ensemble Models**\n",
    "\n",
    "OOB samples serve as **unseen data** for each model in the ensemble.  \n",
    "Since these samples are not used in the training of a particular model, they can be used to test that model‚Äôs performance, similar to a **validation set**.\n",
    "\n",
    "This process allows us to estimate the performance of the ensemble model **without using a separate validation or test dataset**.\n",
    "\n",
    "\n",
    "\n",
    "##  **OOB Score ‚Äì Definition and Purpose**\n",
    "\n",
    "The **Out-of-Bag (OOB) Score** is an internal validation score used to evaluate the performance of ensemble models like **Random Forest**.  \n",
    "It is calculated by using the OOB samples to test the prediction accuracy of the corresponding model.\n",
    "\n",
    "### **How the OOB Score is Computed:**\n",
    "\n",
    "1. For each data point in the dataset, identify all the models (trees) for which it was an OOB sample (i.e., it was not used in that tree‚Äôs training).  \n",
    "2. Predict the class (or value) of that data point using only those trees where it was OOB.  \n",
    "3. Compare the predicted label to the true label.  \n",
    "4. The overall OOB score is computed as the **average accuracy (or error rate)** of these predictions over all samples.\n",
    "\n",
    "This gives a reliable measure of model performance similar to cross-validation, but without needing to split the dataset manually.\n",
    "\n",
    "---\n",
    "\n",
    "##  **Formula for OOB Score**\n",
    "\n",
    "If $N$ is the total number of data points, and $I(\\hat{y}_i = y_i)$ is an indicator function that is 1 if the predicted value equals the true value, then:\n",
    "\n",
    "$$\n",
    "\\text{OOB Score} = \\frac{1}{N} \\sum_{i=1}^{N} I(\\hat{y}_i = y_i)\n",
    "$$\n",
    "\n",
    "where $\\hat{y}_i$ is the predicted output from the OOB models for sample $i$.\n",
    "\n",
    ".\n",
    "\n",
    "\n",
    "\n",
    "##  **Advantages of Using OOB Score**\n",
    "\n",
    "- **No Need for a Separate Validation Set:**  \n",
    "  OOB samples act as an internal test set, saving data and computation.\n",
    "\n",
    "- **Efficient Model Evaluation:**  \n",
    "  Provides an unbiased estimate of the model‚Äôs performance without cross-validation.\n",
    "\n",
    "- **Automatic Error Estimation:**  \n",
    "  Random Forest and other Bagging models can directly provide OOB score values during training.\n",
    "\n",
    "- **Time-Saving:**  \n",
    "  Since OOB evaluation happens during training, it avoids the need for additional testing steps.\n",
    "\n",
    "\n",
    "\n",
    "##  **Limitations of OOB Score**\n",
    "\n",
    "- Works best with **Bagging-based** methods like Random Forest; not suitable for Boosting methods such as XGBoost or AdaBoost.  \n",
    "- May give slightly optimistic or biased results for very small datasets.  \n",
    "- Accuracy of the OOB score can vary depending on the number of trees and data complexity.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07df2bf2-b7f0-4ac2-a0cf-039fa132d5cb",
   "metadata": {},
   "source": [
    "## Question 5: Compare feature importance analysis in a single Decision Tree vs. a Random Forest.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878c703a-c218-42ed-9725-f810e6c7615a",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "##  **Feature Importance in a Single Decision Tree**\n",
    "\n",
    "In a **Decision Tree**, the concept of *feature importance* measures how much each feature contributes to reducing impurity (or increasing information gain) when making predictions.\n",
    "\n",
    "Each node in the tree represents a **split** on a feature. The quality of a split is determined by how much it reduces a chosen impurity metric, such as:\n",
    "- **Gini Impurity** (for classification)\n",
    "- **Entropy** (for classification)\n",
    "- **Mean Squared Error (MSE)** (for regression)\n",
    "\n",
    "The more a feature helps reduce impurity across all its splits, the **higher its importance score**.\n",
    "\n",
    "### üîπ How Feature Importance is Calculated in a Decision Tree:\n",
    "1. At each split, compute the **decrease in impurity** due to that split.\n",
    "2. Assign this decrease to the feature used for the split.\n",
    "3. Sum up the total decrease in impurity for each feature across all splits.\n",
    "4. Normalize the total importance scores so that they sum up to **1**.\n",
    "\n",
    "Mathematically, the importance of feature \\( j \\) can be expressed as:\n",
    "\n",
    "$$\n",
    "FI_j = \\frac{\\sum_{t \\in T_j} \\Delta I(t)}{\\sum_{k} \\sum_{t \\in T_k} \\Delta I(t)}\n",
    "$$\n",
    "\n",
    "Where:  \n",
    "- \\( \\Delta I(t) \\): decrease in impurity at node \\( t \\).  \n",
    "- \\( T_j \\): set of nodes where feature \\( j \\) is used for splitting.  \n",
    "\n",
    "In short, **a feature‚Äôs importance** in a Decision Tree depends solely on how much it helps split the data effectively in that tree.\n",
    "\n",
    "\n",
    "\n",
    "##  **Feature Importance in a Random Forest**\n",
    "\n",
    "A **Random Forest** is an ensemble of multiple Decision Trees trained on different bootstrap samples and random subsets of features.  \n",
    "Feature importance in Random Forests is **averaged across all trees** to get a more reliable and generalized measure.\n",
    "\n",
    "Each tree gives its own estimate of feature importance (like in a single Decision Tree), and the Random Forest combines them to reduce noise and overfitting.\n",
    "\n",
    "###  How Feature Importance is Calculated in a Random Forest:\n",
    "1. Train multiple Decision Trees on different random subsets of data and features.\n",
    "2. Compute the feature importance for each tree (using impurity decrease or other metrics).\n",
    "3. Take the **average of all importance scores** across trees for each feature.\n",
    "\n",
    "Formally, the feature importance for feature \\( j \\) in a Random Forest is:\n",
    "\n",
    "$$\n",
    "FI_j^{RF} = \\frac{1}{M} \\sum_{m=1}^{M} FI_{j}^{(m)}\n",
    "$$\n",
    "\n",
    "Where:  \n",
    "- \\( M \\): total number of trees in the forest  \n",
    "- \\( FI_{j}^{(m)} \\): importance of feature \\( j \\) in tree \\( m \\)  \n",
    "\n",
    "This aggregation process gives a more **robust and stable estimate** of feature importance compared to a single tree.\n",
    "\n",
    "\n",
    "\n",
    "##  **Comparison Between Decision Tree and Random Forest**\n",
    "| **Aspect** | **Decision Tree** | **Random Forest** |\n",
    "|-------------|-------------------|-------------------|\n",
    "| **Model Type** | Single model | Ensemble of multiple Decision Trees |\n",
    "| **Computation Basis** | Based on impurity reduction in one tree | Average of impurity reduction across all trees |\n",
    "| **Stability** | May be unstable (small data changes can alter importance) | More stable and reliable due to averaging |\n",
    "| **Bias & Variance** | High variance (sensitive to noise) | Low variance (ensemble reduces noise) |\n",
    "| **Overfitting** | Prone to overfitting | Less prone to overfitting |\n",
    "| **Interpretability** | Easier to interpret | Harder to interpret (many trees) |\n",
    "| **Use Case** | Good for understanding feature relationships in small datasets | Better for accurate and generalized feature importance estimation |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d2f034-4590-426b-8a2a-69e285c5d40c",
   "metadata": {},
   "source": [
    "## Question 6: Write a Python program to:\n",
    "## ‚óè Load the Breast Cancer dataset using\n",
    "# sklearn.datasets.load_breast_cancer()\n",
    "## ‚óè Train a Random Forest Classifier\n",
    "## ‚óè Print the top 5 most important features based on feature importance scores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5775ed0e-2e3e-4ebd-9a6a-672445c20ceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 Most Important Features:\n",
      "\n",
      "                 Feature  Importance\n",
      "23            worst area    0.139357\n",
      "27  worst concave points    0.132225\n",
      "7    mean concave points    0.107046\n",
      "20          worst radius    0.082848\n",
      "22       worst perimeter    0.080850\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Import necessary libraries\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Step 2: Load the Breast Cancer dataset\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "feature_names = data.feature_names\n",
    "\n",
    "# Step 3: Create and train the Random Forest Classifier\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X, y)\n",
    "\n",
    "# Step 4: Get feature importance scores\n",
    "importances = rf.feature_importances_\n",
    "\n",
    "# Step 5: Create a DataFrame to display features and their importance\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': importances\n",
    "})\n",
    "\n",
    "# Step 6: Sort features by importance in descending order\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Step 7: Print the top 5 most important features\n",
    "print(\"Top 5 Most Important Features:\\n\")\n",
    "print(feature_importance_df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3378d1dd-9f5f-4072-b748-d03b232e3140",
   "metadata": {},
   "source": [
    "## Question 7: Write a Python program to:\n",
    "## ‚óè Train a Bagging Classifier using Decision Trees on the Iris dataset\n",
    "## ‚óè Evaluate its accuracy and compare with a single Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14369c59-1d1c-4d27-8b7a-3600d29d2769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Single Decision Tree: 100.00%\n",
      "Accuracy of Bagging Classifier: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Import necessary libraries\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Step 2: Load the Iris dataset\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Step 3: Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Step 4: Train a single Decision Tree classifier\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "dt.fit(X_train, y_train)\n",
    "y_pred_dt = dt.predict(X_test)\n",
    "dt_accuracy = accuracy_score(y_test, y_pred_dt)\n",
    "\n",
    "# Step 5: Train a Bagging Classifier using Decision Trees as base estimators\n",
    "bagging_clf = BaggingClassifier(\n",
    "    estimator=DecisionTreeClassifier(), \n",
    "    n_estimators=100,                    \n",
    "    random_state=42,\n",
    "    n_jobs=-1                            \n",
    ")\n",
    "bagging_clf.fit(X_train, y_train)\n",
    "y_pred_bag = bagging_clf.predict(X_test)\n",
    "bagging_accuracy = accuracy_score(y_test, y_pred_bag)\n",
    "\n",
    "# Step 6: Print and compare accuracies\n",
    "print(\"Accuracy of Single Decision Tree: {:.2f}%\".format(dt_accuracy * 100))\n",
    "print(\"Accuracy of Bagging Classifier: {:.2f}%\".format(bagging_accuracy * 100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87044b99-b2c9-4bce-8d1c-a2f495358a2c",
   "metadata": {},
   "source": [
    "## Question 8: Write a Python program to:\n",
    "## ‚óè Train a Random Forest Classifier\n",
    "## ‚óè Tune hyperparameters max_depth and n_estimators using GridSearchCV\n",
    "## ‚óè Print the best parameters and final accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4d8d425-55fe-45fd-8f44-0b198fd4d7f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters Found: {'max_depth': None, 'n_estimators': 100}\n",
      "Final Accuracy of Best Model: 100.00%\n"
     ]
    }
   ],
   "source": [
    " #Step 1: Import necessary libraries\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Step 2: Load the Iris dataset\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Step 3: Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Step 4: Define the Random Forest Classifier\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Step 5: Define the parameter grid for tuning\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150, 200],  # Number of trees\n",
    "    'max_depth': [None, 5, 10, 15, 20]    # Depth of each tree\n",
    "}\n",
    "\n",
    "# Step 6: Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=rf,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,                # 5-fold cross-validation\n",
    "    scoring='accuracy',  # Metric to optimize\n",
    "    n_jobs=-1            # Use all CPU cores\n",
    ")\n",
    "\n",
    "# Step 7: Fit the model to the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Step 8: Get the best parameters and best model\n",
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Step 9: Evaluate the best model on the test data\n",
    "y_pred = best_model.predict(X_test)\n",
    "final_accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Step 10: Print the results\n",
    "print(\"Best Parameters Found:\", best_params)\n",
    "print(\"Final Accuracy of Best Model: {:.2f}%\".format(final_accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da935a9e-1797-4368-817f-5e83ffb04958",
   "metadata": {},
   "source": [
    "## Question 9: Write a Python program to:\n",
    "## ‚óè Train a Bagging Regressor and a Random Forest Regressor on the California Housing dataset\n",
    "## ‚óè Compare their Mean Squared Errors (MSE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a4a32f9-f2fe-48c5-8d0d-3cd953c5606f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (Bagging Regressor): 0.2568\n",
      "Mean Squared Error (Random Forest Regressor): 0.2565\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Import necessary libraries\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Step 2: Load the California Housing dataset\n",
    "data = fetch_california_housing()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Step 3: Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Step 4: Train a Bagging Regressor using Decision Trees as base estimators\n",
    "bagging_regressor = BaggingRegressor(\n",
    "    estimator=DecisionTreeRegressor(),\n",
    "    n_estimators=100,       # number of trees\n",
    "    random_state=42,\n",
    "    n_jobs=-1               # use all CPU cores\n",
    ")\n",
    "bagging_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Step 5: Train a Random Forest Regressor\n",
    "rf_regressor = RandomForestRegressor(\n",
    "    n_estimators=100,       # number of trees\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Step 6: Make predictions\n",
    "y_pred_bag = bagging_regressor.predict(X_test)\n",
    "y_pred_rf = rf_regressor.predict(X_test)\n",
    "\n",
    "# Step 7: Compute Mean Squared Error (MSE)\n",
    "mse_bag = mean_squared_error(y_test, y_pred_bag)\n",
    "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
    "\n",
    "# Step 8: Print and compare MSE values\n",
    "print(\"Mean Squared Error (Bagging Regressor): {:.4f}\".format(mse_bag))\n",
    "print(\"Mean Squared Error (Random Forest Regressor): {:.4f}\".format(mse_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d2d9a3-5853-4216-9425-8651cd20315d",
   "metadata": {},
   "source": [
    "## Question 10: You are working as a data scientist at a financial institution to predict loan default. You have access to customer demographic and transaction history data. You decide to use ensemble techniques to increase model performance.\n",
    "# Explain your step-by-step approach to:\n",
    "## ‚óè Choose between Bagging or Boosting\n",
    "## ‚óè Handle overfitting\n",
    "## ‚óè Select base models\n",
    "## ‚óè Evaluate performance using cross-validation\n",
    "## ‚óè Justify how ensemble learning improves decision-making in this real-worldcontext.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e0e5b9-cbc7-4d06-9fb0-be1f5f993102",
   "metadata": {},
   "source": [
    "Ensemble approach for predicting loan default (step-by-step)\n",
    "\n",
    "Below is a practical, production-oriented step-by-step approach you can use at a financial institution to build an ensemble model for loan-default prediction. It covers **how to choose between bagging vs boosting**, **how to handle overfitting**, **how to pick base models**, **how to evaluate with cross-validation**, and **how ensemble learning improves decision-making** ‚Äî with actionable recommendations and pointers to best practices.\n",
    "\n",
    "---\n",
    "\n",
    "## 0) First things first ‚Äî constraints & data checklist\n",
    "Before modeling, gather and document:\n",
    "- business objective (e.g., reduce default rate while keeping reasonable acceptance)  \n",
    "- regulatory requirements (explainability, fairness, data retention) ‚Äî *document these up front*. :contentReference[oaicite:0]{index=0}  \n",
    "- taxonomy of variables: demographics, credit bureau, transactions, repayment history, derived features (e.g., rolling delinquency).  \n",
    "- label definition and look-ahead window (what counts as ‚Äúdefault‚Äù and over what time).  \n",
    "- data quality checks (missingness, duplicates, impossible values) and PSI/CSI monitoring plan.\n",
    "\n",
    "---\n",
    "\n",
    "## 1) Choose between **Bagging** or **Boosting** (how to decide)\n",
    "\n",
    "**Start from the error profile and business needs**:\n",
    "\n",
    "- If your baseline models show **high variance** (unstable predictions, high sensitivity to training samples), **bagging / Random Forest** is a good first choice because it reduces variance by averaging many decorrelated trees. :contentReference[oaicite:1]{index=1}  \n",
    "- If the baseline shows **high bias** (underfitting ‚Äî model is consistently missing patterns) or you need **very high predictive power** to improve ranking (e.g., maximize separation between good and bad), **boosting** (gradient-boosted trees like XGBoost/LightGBM/CatBoost) often yields higher accuracy. Many credit-risk studies report strong performance for boosting while noting interpretability tradeoffs. :contentReference[oaicite:2]{index=2}  \n",
    "- Regulatory / explainability constraints: if regulators require easier model explanation and you can accept slightly lower predictive performance, prefer simpler models or augment complex ensembles with strong explainability (SHAP) and governance. :contentReference[oaicite:3]{index=3}\n",
    "\n",
    "**Practical rule of thumb**\n",
    "1. Try a **Random Forest** first for a robust baseline.  \n",
    "2. If you need better ranking/ROC/PR and can support model governance & explanation, try **gradient boosting** and compare.  \n",
    "3. If both variance and bias are problems, consider **stacking** (blend RF + boosting + simpler models) with a meta-learner.\n",
    "\n",
    "---\n",
    "\n",
    "## 2) Handle overfitting (data + modeling + post-modeling controls)\n",
    "\n",
    "**Data / feature level**\n",
    "- Use *temporal train/validation split* (no leakage): train on past slices, validate on later time slices. Never randomly split across time for risk models.  \n",
    "- Feature sanitization: remove features that leak future info (e.g., post-application balances).  \n",
    "- Regularize feature space: limit highly correlated engineered variables or use PCA/feature selection if necessary.\n",
    "\n",
    "**Model level**\n",
    "- **For Decision-tree ensembles**:\n",
    "  - Random Forest: limit `max_depth`, `min_samples_leaf`, `max_features` to reduce overfitting. :contentReference[oaicite:4]{index=4}  \n",
    "  - Boosting: tune `learning_rate` (shrinkage), `n_estimators` and `max_depth`; smaller learning rate + more trees often generalizes better. :contentReference[oaicite:5]{index=5}\n",
    "- **Cross-validation**: use *time-aware* CV (rolling/expanding window) to replicate production drift. See next section for details.\n",
    "\n",
    "**Resampling & class imbalance**\n",
    "- Loan default datasets are often imbalanced. Prefer **thresholding, class weights, or sampling** rather than blind oversampling. Try `class_weight='balanced'` or calibrate decision threshold using business cost matrix. If using oversampling, do it only inside CV folds (no leakage). :contentReference[oaicite:6]{index=6}\n",
    "\n",
    "**Post-model calibration and monitoring**\n",
    "- Calibrate probabilities (Platt scaling or isotonic) if you need well-calibrated default probabilities for pricing or provisioning.  \n",
    "- Put model performance & population stability (PSI) monitoring into production to catch drift early. :contentReference[oaicite:7]{index=7}\n",
    "\n",
    "---\n",
    "\n",
    "## 3) Select base models (what models to include and why)\n",
    "\n",
    "**Good candidate base models**\n",
    "- **Logistic Regression (regularized)** ‚Äî strong baseline, transparent, often used in production credit scoring.  \n",
    "- **Decision Tree** ‚Äî useful for interpretability & quick diagnostics.  \n",
    "- **Random Forest** ‚Äî bagging ensemble, robust baseline.  \n",
    "- **Gradient Boosted Trees (XGBoost / LightGBM / CatBoost)** ‚Äî state-of-the-art predictive performance for tabular credit data.  \n",
    "- **Simple rule/model** (e.g., rule-based score) ‚Äî useful as a comparator and for explainability.\n",
    "\n",
    "**How to combine**\n",
    "- If you choose **Bagging**: base estimator = tree (unstable learner); use many trees.  \n",
    "- If you choose **Boosting**: base estimator typically shallow trees (depth 3‚Äì8).  \n",
    "- For **Stacking**: combine heterogeneous learners (e.g., logistic + RF + GBM) and train a small meta-learner (logistic/regression) on out-of-fold predictions. This often improves robustness to different error modes.\n",
    "\n",
    "**Practical notes**\n",
    "- Prefer tree-based models for raw tabular/transaction features ‚Äî they handle categorical splits and missingness (CatBoost handles categoricals natively).  \n",
    "- Use regularized logistic regression on engineered, risk-policy features if you need a simple explainable fallback.\n",
    "\n",
    "---\n",
    "\n",
    "## 4) Evaluate performance using cross-validation (recommended strategy)\n",
    "\n",
    "**Use time-aware CV**\n",
    "- For credit risk, use **temporal (rolling/expanding window) cross-validation** instead of random k-fold CV to prevent leakage and mimic real-world deployment. Example: train on months 1‚Äì12, validate on month 13; roll forward. This helps estimate model stability over time.\n",
    "\n",
    "**Metrics: choose business-relevant metrics**\n",
    "- **Ranking metrics**: ROC-AUC (good general metric), **Gini** (2*AUC‚àí1) ‚Äî commonly reported in credit. :contentReference[oaicite:8]{index=8}  \n",
    "- **Imbalance-sensitive metrics**: Precision-Recall AUC (PR-AUC), F1, and recall at a fixed precision (or vice versa) ‚Äî useful when defaults are rare and false negatives (missed defaults) are costly. :contentReference[oaicite:9]{index=9}  \n",
    "- **Business metrics**: Expected Loss, Profit / cost matrix (cost of false approve vs false reject), acceptance rate, and population stability (PSI) for monitoring. :contentReference[oaicite:10]{index=10}  \n",
    "- **Calibration**: Brier score or calibration plots if probabilities are used for pricing/provisioning.\n",
    "\n",
    "**Cross-validation workflow**\n",
    "1. Define time windows for training/validation (rolling).  \n",
    "2. For each fold: do full preprocessing inside training fold (impute, scale, encode), fit model, produce predictions on validation fold.  \n",
    "3. Aggregate fold metrics (AUC, PR-AUC, F1, business metric).  \n",
    "4. Use nested CV or separate tuning validation to avoid optimistic hyperparameter selection.\n",
    "\n",
    "**Hyperparameter tuning**\n",
    "- Use GridSearchCV or RandomizedSearch with *time-aware CV* (custom CV splitter) or use `skopt`/BayesOpt to reduce compute. Validate candidate models on a holdout temporal window before final retrain.\n",
    "\n",
    "---\n",
    "\n",
    "## 5) Explainability, fairness and regulatory compliance\n",
    "\n",
    "**Explainability tools**\n",
    "- Use **SHAP** for local and global explanations (feature contributions per applicant and overall feature importance). SHAP is widely used in credit risk but be mindful that SHAP values can be unstable with heavy imbalance or correlated features ‚Äî check stability. :contentReference[oaicite:11]{index=11}  \n",
    "- Provide **model cards**, decision-flow documentation, and feature governance registers for auditors/regulators. Regulatory bodies advocate for transparency and model monitoring. :contentReference[oaicite:12]{index=12}\n",
    "\n",
    "**Fairness**\n",
    "- Test for disparate impact across protected groups (gender, race, etc.) and document mitigation steps. If mitigation is required, you may prefer simpler transparent models or use post-hoc adjustments, subject to legal review.\n",
    "\n",
    "---\n",
    "\n",
    "## 6) How to implement & validate in production (practical checklist)\n",
    "\n",
    "1. **Data pipeline**: reproducible feature engineering; absolute versioning of feature code.  \n",
    "2. **Model pipeline**: training pipeline (preprocessing ‚Üí model ‚Üí calibration ‚Üí explainers) serialized with versions.  \n",
    "3. **Backtesting**: simulate decisions on historical time windows and compute business KPIs (losses, approvals).  \n",
    "4. **A/B testing / shadow mode**: run model in shadow to compare live performance before full rollout.  \n",
    "5. **Monitoring**: track model accuracy, PSI, feature distributions, reject/accept rates, and economic KPIs.  \n",
    "6. **Governance**: create retrain triggers (drift thresholds), retrain cadence, and human-in-the-loop review for borderline cases.\n",
    "\n",
    "---\n",
    "\n",
    "## 7) Why ensemble learning improves decision-making in this real-world context\n",
    "\n",
    "- **Better predictive performance:** Ensembles (especially boosting) often give higher AUC/PR-AUC, improving the bank‚Äôs ability to separate likely defaulters from good customers ‚Äî this directly improves risk selection and reduces losses. :contentReference[oaicite:13]{index=13}  \n",
    "- **Reduced variance & robustness:** Bagging (Random Forest) reduces sensitivity to noisy data and idiosyncratic features, which stabilizes decisions across cohorts and time. :contentReference[oaicite:14]{index=14}  \n",
    "- **Combining strengths:** Stacking allows combining simple transparent models (for governance) with powerful learners (for ranking), yielding both performance and explainability.  \n",
    "- **Actionable explanations:** With SHAP and robust governance, you can produce per-application explanations that can be used in customer communications and regulatory reporting. :contentReference[oaicite:15]{index=15}\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a526b12-2a3c-4446-ae23-afd885eb3e66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
