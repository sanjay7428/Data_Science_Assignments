{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60f3690d-65d3-48c4-b747-3c8c185b2fe4",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2428074-4229-4ecb-9e3b-462d2b911397",
   "metadata": {},
   "source": [
    "## 1. What is a parameter?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6edffccc-ea62-414f-865f-8d72cfe26736",
   "metadata": {},
   "source": [
    "Parameter in Feature Engineering / Machine Learning\n",
    "\n",
    "A parameter is a value that defines the behavior of a model or feature transformation and is learned from the data during training.\n",
    "\n",
    "Parameters are internal to the model (like weights and biases in a neural network).\n",
    "\n",
    "They are optimized automatically to minimize the loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b410c42-0036-4f96-a662-1869dfac8723",
   "metadata": {},
   "source": [
    "## 2. What is correlation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600d936c-0bd1-49fc-9798-708afa6670a8",
   "metadata": {},
   "source": [
    "In statistics and data analysis, correlation measures the strength and direction of a relationship between two variables.\n",
    "\n",
    "Key Points:\n",
    "\n",
    "Range:\n",
    "\n",
    "Correlation values (denoted as r) range from -1 to +1.\n",
    "\n",
    "r=+1 → Perfect positive correlation (both variables increase together)\n",
    "\n",
    "r=−1 → Perfect negative correlation (one increases, the other decreases)\n",
    "\n",
    "r=0 → No linear correlation\n",
    "\n",
    "Types of Correlation:\n",
    "\n",
    "Positive Correlation: Both variables move in the same direction.\n",
    "\n",
    "Example: Height and weight\n",
    "\n",
    "Negative Correlation: Variables move in opposite directions.\n",
    "\n",
    "Example: Speed of a car and time taken to reach a destination\n",
    "\n",
    "No Correlation: Variables are independent or unrelated.\n",
    "\n",
    "Methods to Measure Correlation:\n",
    "\n",
    "Pearson correlation: Measures linear relationship between continuous variables.\n",
    "\n",
    "Spearman correlation: Measures monotonic relationship (does not require linearity).\n",
    "\n",
    "Kendall correlation: Another rank-based correlation method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35f113a-2866-4236-afe2-cbe3cbe22c50",
   "metadata": {},
   "source": [
    "## What does negative correlation mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7319c91a-32e8-428d-96a7-89344c0479a7",
   "metadata": {},
   "source": [
    "Negative correlation means that two variables move in opposite directions.\n",
    "\n",
    "When one variable increases, the other variable decreases.\n",
    "\n",
    "The correlation coefficient r is less than 0 (−1 ≤ r < 0).\n",
    "\n",
    "Key Points:\n",
    "\n",
    "Range:\n",
    "\n",
    "r=−1 → Perfect negative correlation (exact opposite relationship)\n",
    "\n",
    "r=0 → No linear relationship\n",
    "\n",
    "r=−0.5 → Moderate negative correlation\n",
    "\n",
    "Interpretation:\n",
    "\n",
    "Stronger negative values (closer to −1) → stronger inverse relationship\n",
    "\n",
    "Closer to 0 → weaker inverse relationship"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18835961-ab27-4cb5-a479-626bc6703305",
   "metadata": {},
   "source": [
    "## 3. Define Machine Learning. What are the main components in Machine Learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a69965-5949-432b-b388-1c6c12d050aa",
   "metadata": {},
   "source": [
    "Main Components of Machine Learning:\n",
    "\n",
    "Data:\n",
    "\n",
    "Raw information used to train and evaluate the model.\n",
    "\n",
    "Can be structured (tables, numbers) or unstructured (images, text, audio).\n",
    "\n",
    "Features:\n",
    "\n",
    "Individual measurable properties or characteristics of the data.\n",
    "\n",
    "Example: In predicting house prices, features could be number of rooms, area, location.\n",
    "\n",
    "Model:\n",
    "\n",
    "A mathematical representation or algorithm that learns patterns from data.\n",
    "\n",
    "Examples: Linear regression, decision trees, neural networks.\n",
    "\n",
    "Learning Algorithm:\n",
    "\n",
    "The method used to train the model and adjust its parameters to fit the data.\n",
    "\n",
    "Examples: Gradient descent, backpropagation, k-nearest neighbors.\n",
    "\n",
    "Parameters:\n",
    "\n",
    "Internal values learned by the model from data during training (e.g., weights and biases in neural networks).\n",
    "\n",
    "Objective / Loss Function:\n",
    "\n",
    "A function that measures how well the model is performing.\n",
    "\n",
    "The learning algorithm tries to minimize the loss.\n",
    "\n",
    "Evaluation / Metrics:\n",
    "\n",
    "Methods to check the model’s performance.\n",
    "\n",
    "Examples: Accuracy, precision, recall, mean squared error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c828366-402c-48b4-a60d-3ce0baba4328",
   "metadata": {},
   "source": [
    "## 4. How does loss value help in determining whether the model is good or not?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7773b624-cfaa-4b80-b20e-27bc84da68a0",
   "metadata": {},
   "source": [
    "What is a Loss Value?\n",
    "\n",
    "The loss value (or loss function) measures how well a machine learning model’s predictions match the true values.\n",
    "\n",
    "It quantifies the error of the model.\n",
    "\n",
    "Lower loss → predictions are closer to actual values; higher loss → predictions are farther off.\n",
    "\n",
    "How Loss Value Helps Evaluate a Model\n",
    "\n",
    "Model Training:\n",
    "\n",
    "During training, the learning algorithm tries to minimize the loss by adjusting the model’s parameters.\n",
    "\n",
    "Example: In linear regression, minimizing Mean Squared Error (MSE) adjusts the weights to fit the line closely to the data points.\n",
    "\n",
    "Model Comparison:\n",
    "\n",
    "If you have multiple models, the one with lower loss on the validation set is usually better.\n",
    "\n",
    "Example: Comparing two regression models:\n",
    "\n",
    "Model A: MSE = 5\n",
    "\n",
    "Model B: MSE = 12 → Model A is better.\n",
    "\n",
    "Overfitting / Underfitting Detection:\n",
    "\n",
    "Training loss decreases over time.\n",
    "\n",
    "Validation loss helps detect:\n",
    "\n",
    "Underfitting: Both training & validation loss are high → model too simple.\n",
    "\n",
    "Overfitting: Training loss is low, but validation loss is high → model memorized training data, performs poorly on new data.\n",
    "\n",
    "Guides Model Improvement:\n",
    "\n",
    "High loss → need better features, more data, or a different model.\n",
    "\n",
    "Low loss → model is performing well on the training/validation data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32437345-b18c-40bc-9ef0-71dc9226a962",
   "metadata": {},
   "source": [
    "## 5. What are continuous and categorical variables?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72cb81fb-573d-4914-92da-aa1f487e3356",
   "metadata": {},
   "source": [
    "1. Continuous Variables\n",
    "\n",
    "These are numeric variables that can take any value within a range.\n",
    "\n",
    "They are measurable, and you can perform arithmetic operations on them (addition, subtraction, etc.).\n",
    "\n",
    "Often real numbers with decimals.\n",
    "\n",
    "2. Categorical Variables\n",
    "\n",
    "These are variables that represent categories or groups.\n",
    "\n",
    "They are qualitative, not numeric (though sometimes numbers are used as labels).\n",
    "\n",
    "Usually indicate labels, types, or classes.\n",
    "\n",
    "Types of Categorical Variables:\n",
    "\n",
    "Nominal: Categories without any order (e.g., color, gender)\n",
    "\n",
    "Ordinal: Categories with a natural order (e.g., education level: High School < Bachelor < Master)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f971af-f968-40af-a9d9-8f45f0f6b8cd",
   "metadata": {},
   "source": [
    "## 6. How do we handle categorical variables in Machine Learning? What are the common techniques?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a0998f-1132-4898-80f0-663c12da191a",
   "metadata": {},
   "source": [
    "Common Techniques to Handle Categorical Variables\n",
    "1. Label Encoding\n",
    "\n",
    "Assigns a unique integer to each category.\n",
    "\n",
    "Useful for ordinal variables (where order matters).\n",
    "\n",
    "Example:\n",
    "\n",
    "Education Level: High School → 0, Bachelor → 1, Master → 2\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "education = [\"High School\", \"Bachelor\", \"Master\"]\n",
    "encoded = le.fit_transform(education)\n",
    "print(encoded)  # Output: [0 1 2]\n",
    "\n",
    "\n",
    "Note: Not suitable for nominal variables with no order, as it may imply a hierarchy.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ca8b50e-a586-47ac-ac42-bcb6ac068a4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 2]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "education = [\"High School\", \"Bachelor\", \"Master\"]\n",
    "encoded = le.fit_transform(education)\n",
    "print(encoded)  # Output: [0 1 2]\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "08c12de8-738b-4ce6-8eac-4926cb39576e",
   "metadata": {},
   "source": [
    "2. One-Hot Encoding\n",
    "\n",
    "Converts each category into a binary vector (0 or 1).\n",
    "\n",
    "Best for nominal variables.\n",
    "\n",
    "Example:\n",
    "\n",
    "Color: Red, Blue, Green\n",
    "\n",
    "One-Hot Encoding:\n",
    "    | Red | Blue | Green |\n",
    "| --- | ---- | ----- |\n",
    "| 1   | 0    | 0     |\n",
    "| 0   | 1    | 0     |\n",
    "| 0   | 0    | 1     |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4f4339c-7ccb-4fac-a083-395034f28626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 2)\t1.0\n",
      "  (1, 0)\t1.0\n",
      "  (2, 1)\t1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "colors = np.array([[\"Red\"], [\"Blue\"], [\"Green\"]])\n",
    "ohe = OneHotEncoder()\n",
    "encoded = ohe.fit_transform(colors)\n",
    "print(encoded)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d553b46-00b4-46f5-82ac-faa161924892",
   "metadata": {},
   "source": [
    "## 7. What do you mean by training and testing a dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d164bc9a-e1fd-4022-b56d-1290f10d8996",
   "metadata": {},
   "source": [
    "1. Training a Dataset\n",
    "\n",
    "Training a dataset means using a portion of your data to teach the machine learning model how to make predictions or recognize patterns.\n",
    "\n",
    "During training:\n",
    "\n",
    "The model learns the relationships between input features (X) and target/output (Y).\n",
    "\n",
    "Model parameters (like weights in linear regression or neural networks) are adjusted to minimize error (loss function).\n",
    "\n",
    "Example:\n",
    "\n",
    "Predicting house prices:\n",
    "\n",
    "Features: Size, location, number of rooms\n",
    "\n",
    "Target: Price\n",
    "\n",
    "Model uses the training data to learn how these features affect price.\n",
    "\n",
    "2. Testing a Dataset\n",
    "\n",
    "Testing a dataset means using a separate portion of the data (not seen by the model during training) to evaluate the model’s performance.\n",
    "\n",
    "It helps check if the model can generalize to new, unseen data.\n",
    "\n",
    "Common metrics for testing: Accuracy, Mean Squared Error (MSE), Precision, Recall, F1-score, etc.\n",
    "\n",
    "Example:\n",
    "\n",
    "After training the house price model, you give it new houses from the testing dataset.\n",
    "\n",
    "Compare predicted prices with actual prices to see how well the model performs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6705ab7e-60ce-4000-9038-3b5a6a143a91",
   "metadata": {},
   "source": [
    "## 8. What is sklearn.preprocessing?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a933c5c-8c0f-4cbe-a34f-54559b95bdb5",
   "metadata": {},
   "source": [
    "sklearn.preprocessing\n",
    "\n",
    "sklearn.preprocessing is a module in the scikit-learn library in Python.\n",
    "\n",
    "It provides tools and functions to transform or scale your data before feeding it into a machine learning model.\n",
    "\n",
    "Preprocessing is important because raw data often contains different scales, units, or categorical values, and many ML algorithms work better with normalized or standardized data.\n",
    "\n",
    "Common Tasks in sklearn.preprocessing\n",
    "\n",
    "Scaling / Normalization\n",
    "\n",
    "Adjusting feature values to a common scale.\n",
    "\n",
    "Examples:\n",
    "\n",
    "StandardScaler → scales data to zero mean and unit variance\n",
    "\n",
    "MinMaxScaler → scales data to a range between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca59f033-3aa9-4521-beca-8a3d98c62428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.22474487 -1.22474487]\n",
      " [ 0.          0.        ]\n",
      " [ 1.22474487  1.22474487]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "data = np.array([[10, 200],\n",
    "                 [20, 300],\n",
    "                 [30, 400]])\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "print(scaled_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369b6f68-c4d4-44b2-8a83-2ed36275f220",
   "metadata": {},
   "source": [
    "Encoding Categorical Variables\n",
    "\n",
    "Convert categorical features into numeric representations.\n",
    "\n",
    "Examples:\n",
    "\n",
    "OneHotEncoder → converts categories into binary vectors\n",
    "\n",
    "LabelEncoder → assigns unique integers to categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7ff96d4-de49-4d3e-9164-18d707098e54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 2)\t1.0\n",
      "  (1, 0)\t1.0\n",
      "  (2, 1)\t1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "colors = [[\"Red\"], [\"Blue\"], [\"Green\"]]\n",
    "encoder = OneHotEncoder()\n",
    "encoded_colors = encoder.fit_transform(colors)\n",
    "print(encoded_colors)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7238802-e167-416e-ad3b-8dca84d29778",
   "metadata": {},
   "source": [
    "3. Binarization\n",
    "\n",
    "Convert values into 0 or 1 based on a threshold.\n",
    "\n",
    "Useful for transforming features into binary indicators.\n",
    "\n",
    "4. Polynomial Features\n",
    "\n",
    "Generate new features by combining existing features (e.g., x1², x1*x2) for polynomial regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c2385f-8c2e-4c1c-a39c-962a8061bcd2",
   "metadata": {},
   "source": [
    "## 9. What is a Test set?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1ffc05-b071-4d5c-8edd-c1d38812ada4",
   "metadata": {},
   "source": [
    "test set in machine learning is a subset of the dataset that is kept separate from the training data and is used to evaluate the performance of a trained model.\n",
    "\n",
    "Key Points:\n",
    "\n",
    "Purpose:\n",
    "\n",
    "To check how well the model can generalize to new, unseen data.\n",
    "\n",
    "Helps identify if the model is overfitting or underfitting.\n",
    "\n",
    "Characteristics:\n",
    "\n",
    "Not used during training — the model never sees this data while learning.\n",
    "\n",
    "Typically 20–30% of the total dataset, though the exact split can vary.\n",
    "\n",
    "Evaluation Metrics:\n",
    "\n",
    "Regression: Mean Squared Error (MSE), R² score\n",
    "\n",
    "Classification: Accuracy, Precision, Recall, F1-score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04277c7-0f1b-459f-a08b-1bcf36694e01",
   "metadata": {},
   "source": [
    "## 10. How do we split data for model fitting (training and testing) in Python?\n",
    "## How do you approach a Machine Learning problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fe3a2a-ce3f-4de8-87e9-2bcd24369cc5",
   "metadata": {},
   "source": [
    "Splitting Data for Training and Testing in Python\n",
    "\n",
    "In machine learning, we usually split data into training and testing sets (sometimes also a validation set) to train the model and evaluate its performance.\n",
    "\n",
    "Using scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3785ee8d-9abc-44f0-ac8c-2fcd6b2c08ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Features:\n",
      "    feature1  feature2\n",
      "0         1         5\n",
      "7         8         2\n",
      "2         3         6\n",
      "9        10         0\n",
      "4         5         7\n",
      "3         4         2\n",
      "6         7         8\n",
      "Testing Features:\n",
      "    feature1  feature2\n",
      "8         9         9\n",
      "1         2         3\n",
      "5         6         1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# Example dataset\n",
    "data = pd.DataFrame({\n",
    "    'feature1': [1,2,3,4,5,6,7,8,9,10],\n",
    "    'feature2': [5,3,6,2,7,1,8,2,9,0],\n",
    "    'target':   [0,1,0,1,0,1,0,1,0,1]\n",
    "})\n",
    "\n",
    "# Features and target\n",
    "X = data[['feature1', 'feature2']]\n",
    "y = data['target']\n",
    "\n",
    "# Split into training and testing sets (70% train, 30% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "print(\"Training Features:\\n\", X_train)\n",
    "print(\"Testing Features:\\n\", X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4074b544-fc46-4a19-8913-323a28eedf07",
   "metadata": {},
   "source": [
    "Approach to a Machine Learning Problem\n",
    "\n",
    "Here’s a step-by-step approach commonly used in ML projects:\n",
    "\n",
    "1. Define the Problem\n",
    "\n",
    "Understand the goal: classification, regression, clustering, etc.\n",
    "\n",
    "Identify inputs (features) and outputs (target).\n",
    "\n",
    "2. Collect and Explore Data\n",
    "\n",
    "Gather the dataset.\n",
    "\n",
    "Perform exploratory data analysis (EDA): check distributions, missing values, correlations, and outliers.\n",
    "\n",
    "3. Preprocess Data\n",
    "\n",
    "Handle missing values.\n",
    "\n",
    "Encode categorical variables.\n",
    "\n",
    "Scale numeric features (normalization/standardization).\n",
    "\n",
    "Split data into training and testing sets.\n",
    "\n",
    "4. Choose Model(s)\n",
    "\n",
    "Select algorithm(s) suitable for the problem:\n",
    "\n",
    "Regression → Linear Regression, Random Forest Regressor\n",
    "\n",
    "Classification → Logistic Regression, Decision Trees, SVM\n",
    "\n",
    "Clustering → KMeans, DBSCAN\n",
    "\n",
    "5. Train the Model\n",
    "\n",
    "Fit the model on training data.\n",
    "\n",
    "Tune parameters and possibly hyperparameters.\n",
    "\n",
    "6. Evaluate Model\n",
    "\n",
    "Use test data to evaluate performance using metrics like accuracy, F1-score, MSE, R², etc.\n",
    "\n",
    "Check for overfitting and underfitting.\n",
    "\n",
    "7. Improve Model\n",
    "\n",
    "Feature engineering (create better features)\n",
    "\n",
    "Try different algorithms or hyperparameter tuning\n",
    "\n",
    "Use techniques like cross-validation for better generalization\n",
    "\n",
    "8. Deploy / Predict\n",
    "\n",
    "Use the trained model to predict new unseen data in real-world applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34abdaf5-baf1-4229-998e-a4debbc90b38",
   "metadata": {},
   "source": [
    "## 11. Why do we have to perform EDA before fitting a model to the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6f7ae8-68ba-40b2-a851-a7f63348ef8b",
   "metadata": {},
   "source": [
    "What is EDA?\n",
    "\n",
    "Exploratory Data Analysis (EDA) is the process of analyzing and visualizing your dataset before building a machine learning model.\n",
    "It helps you understand the structure, quality, and patterns in the data.\n",
    "\n",
    "Why EDA is Important Before Fitting a Model\n",
    "\n",
    "Identify Missing or Incorrect Data\n",
    "\n",
    "Detect NaNs, null values, or erroneous entries.\n",
    "\n",
    "Example: A dataset with missing age values or negative salaries.\n",
    "\n",
    "Without handling missing/incorrect data → model may give wrong predictions.\n",
    "\n",
    "Understand Feature Distributions\n",
    "\n",
    "Check how numeric variables are distributed (normal, skewed, uniform).\n",
    "\n",
    "Helps decide scaling, transformation, or feature engineering.\n",
    "\n",
    "Detect Outliers\n",
    "\n",
    "Outliers can bias the model and affect metrics like mean squared error.\n",
    "\n",
    "EDA helps you spot and handle them.\n",
    "\n",
    "Check Relationships and Correlations\n",
    "\n",
    "Identify which features are strongly related to the target.\n",
    "\n",
    "Helps select important features and avoid redundant ones.\n",
    "\n",
    "Understand Data Types\n",
    "\n",
    "Detect categorical vs continuous variables.\n",
    "\n",
    "Helps decide encoding methods for ML models.\n",
    "\n",
    "Visualize Patterns\n",
    "\n",
    "Scatter plots, histograms, box plots, heatmaps help uncover hidden patterns.\n",
    "\n",
    "Example: Linear relationship between house size and price.\n",
    "\n",
    "Prevent Model Failures\n",
    "\n",
    "ML models assume certain things about data (e.g., numeric inputs, no missing values).\n",
    "\n",
    "EDA ensures data quality and suitability for the chosen algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8de31c3-48bd-4dee-9bfa-91aaee42da61",
   "metadata": {},
   "source": [
    "## 12. What is correlation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c592c314-ce1a-4c8c-b03f-caf7b4b3c923",
   "metadata": {},
   "source": [
    "In statistics and data analysis, correlation measures the strength and direction of a relationship between two variables.\n",
    "\n",
    "Key Points:\n",
    "\n",
    "Range:\n",
    "\n",
    "Correlation values (denoted as r) range from -1 to +1.\n",
    "\n",
    "r=+1 → Perfect positive correlation (both variables increase together)\n",
    "\n",
    "r=−1 → Perfect negative correlation (one increases, the other decreases)\n",
    "\n",
    "r=0 → No linear correlation\n",
    "\n",
    "Types of Correlation:\n",
    "\n",
    "Positive Correlation: Both variables move in the same direction.\n",
    "\n",
    "Example: Height and weight\n",
    "\n",
    "Negative Correlation: Variables move in opposite directions.\n",
    "\n",
    "Example: Speed of a car and time taken to reach a destination\n",
    "\n",
    "No Correlation: Variables are independent or unrelated.\n",
    "\n",
    "Methods to Measure Correlation:\n",
    "\n",
    "Pearson correlation: Measures linear relationship between continuous variables.\n",
    "\n",
    "Spearman correlation: Measures monotonic relationship (does not require linearity).\n",
    "\n",
    "Kendall correlation: Another rank-based correlation method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77834caf-118d-4074-afd8-2d3070bbc97b",
   "metadata": {},
   "source": [
    "## 13. What does negative correlation mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5752c5de-7907-43d9-bfa0-2bdeb44868d7",
   "metadata": {},
   "source": [
    "Negative correlation means that two variables move in opposite directions.\n",
    "\n",
    "When one variable increases, the other variable decreases.\n",
    "\n",
    "The correlation coefficient r is less than 0 (−1 ≤ r < 0).\n",
    "\n",
    "Key Points:\n",
    "\n",
    "Range:\n",
    "\n",
    "r=−1 → Perfect negative correlation (exact opposite relationship)\n",
    "\n",
    "r=0 → No linear relationship\n",
    "\n",
    "r=−0.5 → Moderate negative correlation\n",
    "\n",
    "Interpretation:\n",
    "\n",
    "Stronger negative values (closer to −1) → stronger inverse relationship\n",
    "\n",
    "Closer to 0 → weaker inverse relationship"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0480735b-5117-456a-8d71-58cb17bd8d48",
   "metadata": {},
   "source": [
    "## 14. How can you find correlation between variables in Python?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27fff3a-c71c-4d0a-beba-9fa4a67b170a",
   "metadata": {},
   "source": [
    "You can find correlation between variables in Python using pandas and NumPy. The most common method is Pearson correlation, though other methods like Spearman or Kendall can also be used.\n",
    "\n",
    "Here’s a detailed explanation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b9979df-41f2-411b-b4a9-e74b71480cec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Height    Weight       Age\n",
      "Height  1.000000  0.990148  0.656532\n",
      "Weight  0.990148  1.000000  0.734572\n",
      "Age     0.656532  0.734572  1.000000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Example dataset\n",
    "data = pd.DataFrame({\n",
    "    'Height': [150, 160, 170, 180, 190],\n",
    "    'Weight': [50, 60, 65, 80, 90],\n",
    "    'Age': [20, 21, 19, 25, 23]\n",
    "})\n",
    "\n",
    "# Compute correlation matrix (default: Pearson)\n",
    "correlation_matrix = data.corr()\n",
    "print(correlation_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9034e867-7e49-426f-96c2-8bc29d6ee3b8",
   "metadata": {},
   "source": [
    "Each value shows the correlation coefficient r between two variables.\n",
    "\n",
    "1 = perfect positive correlation, -1 = perfect negative correlation, 0 = no correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "293bbdde-7565-44bd-bb30-957950259dc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation between height and weight: 0.9901475429766743\n"
     ]
    }
   ],
   "source": [
    "#Using NumPy\n",
    "import numpy as np\n",
    "\n",
    "height = np.array([150, 160, 170, 180, 190])\n",
    "weight = np.array([50, 60, 65, 80, 90])\n",
    "\n",
    "correlation = np.corrcoef(height, weight)[0, 1]\n",
    "print(\"Correlation between height and weight:\", correlation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b369f944-8cf6-4273-89d5-dbf024cfc36f",
   "metadata": {},
   "source": [
    "## 15. What is causation? Explain difference between correlation and causation with an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5d0e8b-8903-426b-85b9-47521037a213",
   "metadata": {},
   "source": [
    "1. What is Causation?\n",
    "\n",
    "Causation (or causal relationship) occurs when one variable directly influences or causes a change in another variable.\n",
    "\n",
    "In other words, a change in X leads to a change in Y.\n",
    "\n",
    "Causation implies a cause-and-effect relationship, not just an association.\n",
    "\n",
    "Example:\n",
    "\n",
    "Smoking → Lung cancer\n",
    "\n",
    "Smoking causes an increased risk of lung cancer.\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ba66390f-a4bc-4713-a8da-74ef56ee33b5",
   "metadata": {},
   "source": [
    "Difference Between Correlation and Causation\n",
    "\n",
    "| Feature             | Correlation                                                               | Causation                                                     |\n",
    "| ------------------- | ------------------------------------------------------------------------- | ------------------------------------------------------------- |\n",
    "| Definition          | Measures **strength and direction of relationship** between two variables | Indicates **cause-and-effect relationship** between variables |\n",
    "| Relationship Type   | Association / Pattern                                                     | Direct influence                                              |\n",
    "| Direction           | Can be positive, negative, or zero                                        | Always directional: cause → effect                            |\n",
    "| Implication         | Does **not** imply one causes the other                                   | Implies that one variable **affects** the other               |\n",
    "| Can occur by chance | Yes                                                                       | No                                                            |\n",
    "| Example             | Ice cream sales ↑, drowning ↑ → correlated but not causal                 | Smoking ↑ → Lung cancer ↑ → causal relationship               |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fce30f8-fe16-4385-9e80-d46226c6d86c",
   "metadata": {},
   "source": [
    "Example to Illustrate the Difference\n",
    "\n",
    "Scenario:\n",
    "\n",
    "Data shows that ice cream sales and drowning incidents are positively correlated.\n",
    "\n",
    "Correlation: High ice cream sales ↔ More drownings (r > 0)\n",
    "\n",
    "Does this mean buying ice cream causes drowning?\n",
    "\n",
    "No.\n",
    "\n",
    "Reason: Both are influenced by a third variable (summer/temperature).\n",
    "\n",
    "Summer → more ice cream sales & more swimming → more drownings\n",
    "\n",
    "Key takeaway:\n",
    "\n",
    "Correlation = Association\n",
    "\n",
    "Causation = Cause-and-effect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5719c00b-76c2-4d90-9a27-d140bb20e7d8",
   "metadata": {},
   "source": [
    "## 16. What is an Optimizer? What are different types of optimizers? Explain each with an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90aacea9-3df1-46ae-8e42-b349a1765722",
   "metadata": {},
   "source": [
    "What is an Optimizer?\n",
    "\n",
    "In machine learning and deep learning, an optimizer is an algorithm that adjusts the model’s parameters (weights and biases) to minimize the loss function during training.\n",
    "\n",
    "The goal of the optimizer is to find the best set of parameters that reduces the error between predicted and actual values.\n",
    "\n",
    "Optimizers are essential in training neural networks, especially when dealing with large datasets and complex models.\n",
    "\n",
    "2. How Optimizers Work\n",
    "\n",
    "The optimizer calculates gradients of the loss function with respect to model parameters using backpropagation.\n",
    "\n",
    "Then it updates the parameters in the direction that reduces the loss.\n",
    "\n",
    "The learning rate determines how big each step is in updating parameters.\n",
    "\n",
    "3. Common Types of Optimizers\n",
    "A. Gradient Descent (GD)\n",
    "\n",
    "Idea: Update parameters in the opposite direction of the gradient of the loss function.\n",
    "\n",
    "Formula:\n",
    "\n",
    "$$\n",
    "\\theta = \\theta - \\eta \\cdot \\nabla L(\\theta)\n",
    "$$\n",
    "\n",
    "\n",
    "where \n",
    "\n",
    "θ = parameters, \n",
    "\n",
    "η = learning rate, \n",
    "\n",
    "∇L(θ) = gradient of loss.\n",
    "\n",
    "Types of Gradient Descent:\n",
    "\n",
    "Batch Gradient Descent: Uses entire dataset to compute gradient.\n",
    "\n",
    "Accurate but slow for large datasets.\n",
    "\n",
    "Stochastic Gradient Descent (SGD): Uses one sample at a time to update.\n",
    "\n",
    "Fast, but updates are noisy.\n",
    "\n",
    "Mini-batch Gradient Descent: Uses small batches of data (common in practice).\n",
    "\n",
    "Balance between speed and stability.\n",
    "\n",
    "B. Momentum\n",
    "\n",
    "Adds memory of previous updates to accelerate convergence.\n",
    "\n",
    "Helps avoid local minima and smooths updates.\n",
    "\n",
    "Update formula:\n",
    "\n",
    "$$\n",
    "v = \\beta v + \\eta \\nabla L(\\theta)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\theta = \\theta - v\n",
    "$$\n",
    "\n",
    "\n",
    "where \n",
    "\n",
    "v = velocity, \n",
    "\n",
    "β = momentum factor (e.g., 0.9)\n",
    "\n",
    "C. AdaGrad (Adaptive Gradient)\n",
    "\n",
    "Adjusts learning rate for each parameter individually based on historical gradients.\n",
    "\n",
    "Parameters with large gradients → smaller learning rate, small gradients → larger learning rate.\n",
    "\n",
    "Good for sparse data.\n",
    "\n",
    "D. RMSProp (Root Mean Square Propagation)\n",
    "\n",
    "Improves AdaGrad by using a moving average of squared gradients to prevent learning rate from decreasing too much.\n",
    "\n",
    "Popular in RNNs and deep learning.\n",
    "\n",
    "E. Adam (Adaptive Moment Estimation)\n",
    "\n",
    "Combines Momentum + RMSProp.\n",
    "\n",
    "Maintains moving averages of both gradients and squared gradients.\n",
    "\n",
    "Widely used because it’s fast and works well in most cases.\n",
    "\n",
    "Update formulas (simplified):\n",
    "\n",
    "$$\n",
    "m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) g_t\n",
    "$$\n",
    "\n",
    "$$\n",
    "v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\theta = \\theta - \\eta \\cdot \\frac{m_t}{\\sqrt{v_t} + \\epsilon}\n",
    "$$\n",
    "\n",
    "\t​\n",
    "\n",
    "\t​"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72289320-9d9e-4b26-867d-a92236a03d6b",
   "metadata": {},
   "source": [
    "## 17. What is sklearn.linear_model ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac3a06a-6b58-44ef-9422-8eb41d8f4e4c",
   "metadata": {},
   "source": [
    "sklearn.linear_model is a module in the scikit-learn library in Python that provides classes and functions to implement linear models for regression and classification problems.\n",
    "\n",
    "1. What is a Linear Model?\n",
    "\n",
    "A linear model tries to predict a target variable (Y) as a linear combination of input features (X).\n",
    "\n",
    "Formula for regression:\n",
    "\n",
    "$$\n",
    "Y = w_1 X_1 + w_2 X_2 + \\cdots + w_n X_n + b\n",
    "$$\n",
    "\n",
    "\n",
    "𝑤𝑖= weights (parameters learned from data)\n",
    "\n",
    "b = bias/intercept"
   ]
  },
  {
   "cell_type": "raw",
   "id": "52e9b37b-2e62-4021-acb3-ab2ded4ef3f0",
   "metadata": {},
   "source": [
    "Common Classes in sklearn.linear_model\n",
    "\n",
    "| Class                            | Purpose                                                 | Example Use Case                                     |\n",
    "| -------------------------------- | ------------------------------------------------------- | ---------------------------------------------------- |\n",
    "| **LinearRegression**             | Predict continuous numeric values                       | Predict house prices, salary prediction              |\n",
    "| **Ridge**                        | Linear regression with L2 regularization                | Avoid overfitting when features are many             |\n",
    "| **Lasso**                        | Linear regression with L1 regularization                | Feature selection (sparse coefficients)              |\n",
    "| **LogisticRegression**           | Predict categorical outcomes (binary/multi-class)       | Spam detection, disease classification               |\n",
    "| **ElasticNet**                   | Combines L1 and L2 regularization                       | When both feature selection and shrinkage are needed |\n",
    "| **SGDRegressor / SGDClassifier** | Linear models trained using Stochastic Gradient Descent | Large datasets, online learning                      |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2816567e-b22e-42eb-a15e-ff747b232ff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for X=6: [5.8]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "\n",
    "# Example data\n",
    "X = np.array([[1], [2], [3], [4], [5]])\n",
    "y = np.array([2, 4, 5, 4, 5])\n",
    "\n",
    "# Create linear regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Fit model on data\n",
    "model.fit(X, y)\n",
    "\n",
    "# Predict\n",
    "predictions = model.predict(np.array([[6]]))\n",
    "print(\"Prediction for X=6:\", predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e5a94d-9a1c-4a64-b468-9473e6342939",
   "metadata": {},
   "source": [
    "## 18. What does model.fit() do? What arguments must be given?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365dd102-751b-46af-8b79-1216f69a566a",
   "metadata": {},
   "source": [
    "model.fit():\n",
    "\n",
    "In scikit-learn, the fit() method is used to train a machine learning model on a given dataset.\n",
    "\n",
    "It learns the patterns from the data by adjusting the model’s parameters.\n",
    "\n",
    "After calling fit(), the model is ready to make predictions using predict()."
   ]
  },
  {
   "cell_type": "raw",
   "id": "38e1a515-ff8e-4150-94a0-e27216cf2f91",
   "metadata": {},
   "source": [
    "model.fit(X, y)\n",
    "#X = Features / input data\n",
    "\n",
    "#y = Target / output labels\n",
    "\n",
    "| Argument | Description                                                                             |\n",
    "| -------- | --------------------------------------------------------------------------------------- |\n",
    "| X    | Input data (2D array or DataFrame), shape = `[n_samples, n_features]`                   |\n",
    "| y    | Target data (1D array, Series, or 2D array for multiple outputs), shape = `[n_samples]` |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943df0d5-3c0f-40a4-8afa-9bfb27c8b402",
   "metadata": {},
   "source": [
    "## 19. What does model.predict() do? What arguments must be given?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e0c9d8-77f6-4ff3-b5dc-e8a6476140e7",
   "metadata": {},
   "source": [
    "model.predict():\n",
    "\n",
    "In scikit-learn, the predict() method is used to make predictions on new data using a trained machine learning model.\n",
    "\n",
    "You first train the model using model.fit() on your dataset.\n",
    "\n",
    "Then, model.predict() uses the learned parameters (like weights, coefficients, or decision boundaries) to predict output values for new inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2cc46a-59a7-456c-8987-2020abc26e04",
   "metadata": {},
   "source": [
    "predictions = model.predict(X_new)\n",
    "\n",
    "X_new = New input data (features) you want predictions for.\n",
    "\n",
    "The shape of X_new should match the features used during training: [n_samples, n_features]\n",
    "\n",
    "X: Input data for which you want predictions. Must have same number of features as the training data.                                                       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53ce527-f84e-4495-a711-df56f5e5b266",
   "metadata": {},
   "source": [
    "## 20. What are continuous and categorical variables?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1ef05e-0b68-4754-9c70-925ae6b54741",
   "metadata": {},
   "source": [
    "Continuous Variables\n",
    "\n",
    "Continuous variables are numeric variables that can take any value within a range. They are measurable, and you can perform arithmetic operations on them like addition, subtraction, or calculating the mean. Examples include height (like 150.5 cm or 172.3 cm), weight (55.2 kg, 70.8 kg), temperature (36.6°C, 37.4°C), or age (25 years, 30.5 years). Continuous variables can have fractional values and are suitable for statistical calculations like mean, variance, and correlation.\n",
    "\n",
    "Categorical Variables\n",
    "\n",
    "Categorical variables represent categories or groups. They are qualitative and often indicate labels, types, or classes. Examples include gender (Male, Female), color (Red, Blue, Green), vehicle type (Car, Bike, Truck), or blood group (A, B, AB, O). Categorical variables can be nominal, where categories have no order (like color or gender), or ordinal, where categories have a natural order (like education level: High School < Bachelor < Master)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791f332e-9ffd-4f30-a4f5-24cb452535d8",
   "metadata": {},
   "source": [
    "## 21. What is feature scaling? How does it help in Machine Learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4da9b7-e61d-41c9-abb4-79da0d1c287d",
   "metadata": {},
   "source": [
    "Feature Scaling?\n",
    "\n",
    "Feature scaling is the process of standardizing or normalizing the range of independent variables (features) in a dataset so that they are on a similar scale.\n",
    "\n",
    "Different features may have different units or ranges, e.g., height in centimeters (150–200) vs. income in dollars (1000–100000).\n",
    "\n",
    "Feature scaling ensures that all features contribute equally to the learning process.\n",
    "\n",
    "2. Common Feature Scaling Techniques\n",
    "\n",
    "Min-Max Scaling (Normalization)\n",
    "\n",
    "Scales data to a fixed range, usually 0 to 1.\n",
    "\n",
    "Formula:\n",
    "\n",
    "$$\n",
    "X_{\\text{scaled}} = \\frac{X - X_{\\min}}{X_{\\max} - X_{\\min}}\n",
    "$$\n",
    "\n",
    "\n",
    "Standardization (Z-score Scaling)\n",
    "\n",
    "Centers data around zero and scales to unit variance.\n",
    "\n",
    "Formula:\n",
    "\n",
    "$$\n",
    "X_{\\text{scaled}} = \\frac{X - \\mu}{\\sigma}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "where \n",
    "\n",
    "μ = mean, \n",
    "\n",
    "σ = standard deviation\n",
    "\n",
    "MaxAbs Scaling\n",
    "\n",
    "Scales values to the range [-1, 1] based on the maximum absolute value.\n",
    "\n",
    "Robust Scaling\n",
    "\n",
    "Uses median and interquartile range to scale features, useful for datasets with outliers.\n",
    "\n",
    "#Feature Scaling Helps in Machine Learning\n",
    "\n",
    "Faster Convergence:\n",
    "\n",
    "Gradient-based algorithms (like linear regression, logistic regression, neural networks) converge faster when features are on a similar scale.\n",
    "\n",
    "Prevents Feature Dominance:\n",
    "\n",
    "Features with larger numeric ranges can dominate the learning process if not scaled.\n",
    "\n",
    "Improves Distance-Based Models:\n",
    "\n",
    "Algorithms like K-Nearest Neighbors (KNN), K-Means, and SVM rely on distances.\n",
    "\n",
    "Scaling ensures that all features contribute equally to distance calculations.\n",
    "\n",
    "Handles Regularization Better:\n",
    "\n",
    "Models like Ridge, Lasso, ElasticNet perform better when features are scaled, because regularization penalizes large coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d815b68d-bed2-4063-8c18-52e622740f76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardized Data:\n",
      " [[-1.22474487 -1.22474487]\n",
      " [ 0.          0.        ]\n",
      " [ 1.22474487  1.22474487]]\n",
      "Normalized Data:\n",
      " [[0.  0. ]\n",
      " [0.5 0.5]\n",
      " [1.  1. ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "# Sample data\n",
    "X = np.array([[150, 2000],\n",
    "              [160, 3000],\n",
    "              [170, 4000]])\n",
    "\n",
    "# Standardization\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "print(\"Standardized Data:\\n\", X_scaled)\n",
    "\n",
    "# Min-Max Scaling\n",
    "scaler = MinMaxScaler()\n",
    "X_normalized = scaler.fit_transform(X)\n",
    "print(\"Normalized Data:\\n\", X_normalized)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa97e0bc-b4e4-4ade-bea7-1806f503358c",
   "metadata": {},
   "source": [
    "## 22. How do we perform scaling in Python?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7e61fc-4456-48c7-afe4-593206897d67",
   "metadata": {},
   "source": [
    "We can perform feature scaling in Python mainly using scikit-learn’s preprocessing module. The most common techniques are Standardization and Normalization (Min-Max Scaling). Here’s how to do it:\n",
    "\n",
    "1. Using StandardScaler (Z-score Standardization)\n",
    "\n",
    "Scales data to have mean = 0 and standard deviation = 1.\n",
    "\n",
    "Useful for most ML algorithms like linear regression, logistic regression, SVM, neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "112ed302-7d7f-4186-ad14-f302ef15f52d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardized Data:\n",
      " [[-1.22474487 -1.22474487]\n",
      " [ 0.          0.        ]\n",
      " [ 1.22474487  1.22474487]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# Sample data: 3 samples, 2 features\n",
    "X = np.array([[150, 2000],\n",
    "              [160, 3000],\n",
    "              [170, 4000]])\n",
    "\n",
    "# Create a scaler object\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler and transform the data\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "print(\"Standardized Data:\\n\", X_scaled)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3c1c00-815f-4e2e-bba1-2bfe33265582",
   "metadata": {},
   "source": [
    "2. Using MinMaxScaler (Normalization)\n",
    "\n",
    "Scales features to a fixed range, usually 0 to 1.\n",
    "\n",
    "Formula:\n",
    "$$\n",
    "X_{\\text{scaled}} = \\frac{X - X_{\\min}}{X_{\\max} - X_{\\min}}\n",
    "$$\n",
    "\n",
    "Useful for distance-based algorithms like KNN, K-Means, and Neural Networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a69375e7-6d97-49ba-93a2-04d0e5c74c6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized Data:\n",
      " [[0.  0. ]\n",
      " [0.5 0.5]\n",
      " [1.  1. ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_normalized = scaler.fit_transform(X)\n",
    "\n",
    "print(\"Normalized Data:\\n\", X_normalized)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9a5079-c354-4ef3-8e04-be359a20ae64",
   "metadata": {},
   "source": [
    "3. Using RobustScaler\n",
    "\n",
    "Scales using median and interquartile range, good for data with outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e59a7fb9-409a-4a8a-9c1e-2dedc35d9211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Robust Scaled Data:\n",
      " [[-1. -1.]\n",
      " [ 0.  0.]\n",
      " [ 1.  1.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "scaler = RobustScaler()\n",
    "X_robust = scaler.fit_transform(X)\n",
    "\n",
    "print(\"Robust Scaled Data:\\n\", X_robust)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a615c4-9148-4cb4-9855-08df608bdbb7",
   "metadata": {},
   "source": [
    "## 23. What is sklearn.preprocessing?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1f1902-87fe-49f8-ae2f-9fe9b5e9f180",
   "metadata": {},
   "source": [
    "sklearn.preprocessing is a module in the scikit-learn library that provides tools for preprocessing and transforming data before using it in machine learning models.\n",
    "\n",
    "Purpose\n",
    "\n",
    "Raw data often has different scales, units, or types, which can affect the performance of ML models. The preprocessing module helps to normalize, standardize, encode, or scale features so the model can learn effectively.\n",
    "\n",
    "Common Tasks in sklearn.preprocessing\n",
    "\n",
    "Scaling / Normalization\n",
    "\n",
    "Ensures all features are on a similar scale.\n",
    "\n",
    "Examples:\n",
    "\n",
    "StandardScaler → scales features to zero mean and unit variance\n",
    "\n",
    "MinMaxScaler → scales features to a range between 0 and 1\n",
    "\n",
    "Encoding Categorical Variables\n",
    "\n",
    "Converts text or categorical data into numbers.\n",
    "\n",
    "Examples:\n",
    "\n",
    "LabelEncoder → converts categories to integers\n",
    "\n",
    "OneHotEncoder → converts categories to binary vectors\n",
    "\n",
    "Binarization\n",
    "\n",
    "Converts numerical values into 0 or 1 based on a threshold.\n",
    "\n",
    "Example: transforming exam scores into pass/fail indicators\n",
    "\n",
    "Polynomial Features\n",
    "\n",
    "Generates new features by combining existing ones for polynomial regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "17012c98-201f-44d2-b9d7-723935eba717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled Data:\n",
      " [[-1.22474487 -1.22474487]\n",
      " [ 0.          0.        ]\n",
      " [ 1.22474487  1.22474487]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "X = np.array([[150, 2000],\n",
    "              [160, 3000],\n",
    "              [170, 4000]])\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "print(\"Scaled Data:\\n\", X_scaled)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e164f73e-7ecf-4bc7-9384-d9a614c943ca",
   "metadata": {},
   "source": [
    "## 24. How do we split data for model fitting (training and testing) in Python?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b1a485-03f4-4c3e-9ca5-1837a3aea028",
   "metadata": {},
   "source": [
    "In Python, we typically split data into training and testing sets using scikit-learn’s train_test_split function. This is an essential step to train a model on one part of the data and evaluate it on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8df54897-6a57-44f3-83cf-fa96eedfc20a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Features:\n",
      "    feature1  feature2\n",
      "0         1         5\n",
      "7         8         2\n",
      "2         3         6\n",
      "9        10         0\n",
      "4         5         7\n",
      "3         4         2\n",
      "6         7         8\n",
      "Testing Features:\n",
      "    feature1  feature2\n",
      "8         9         9\n",
      "1         2         3\n",
      "5         6         1\n"
     ]
    }
   ],
   "source": [
    "#Using train_test_split\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# Example dataset\n",
    "data = pd.DataFrame({\n",
    "    'feature1': [1,2,3,4,5,6,7,8,9,10],\n",
    "    'feature2': [5,3,6,2,7,1,8,2,9,0],\n",
    "    'target':   [0,1,0,1,0,1,0,1,0,1]\n",
    "})\n",
    "\n",
    "# Features and target\n",
    "X = data[['feature1', 'feature2']]\n",
    "y = data['target']\n",
    "\n",
    "# Split data: 70% training, 30% testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "print(\"Training Features:\\n\", X_train)\n",
    "print(\"Testing Features:\\n\", X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7596e069-53d4-4bea-b2ab-867ca8a3ad79",
   "metadata": {},
   "source": [
    "Key Parameters\n",
    "\n",
    "X → Input features (2D array or DataFrame)\n",
    "\n",
    "y → Target variable (1D array, Series)\n",
    "\n",
    "test_size → Proportion of data for testing (e.g., 0.2 = 20%)\n",
    "\n",
    "train_size → Proportion of data for training (optional, complementary to test_size)\n",
    "\n",
    "random_state → Seed for reproducibility (ensures the same split every time)\n",
    "\n",
    "shuffle → Whether to shuffle data before splitting (default: True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be24c5c5-2412-4a77-b2e3-2e1c2a30b9bc",
   "metadata": {},
   "source": [
    "## 25. Explain data encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc288e70-837d-4694-9bb4-2590c40562a1",
   "metadata": {},
   "source": [
    "Data Encoding:\n",
    "\n",
    "Data encoding is the process of transforming categorical (non-numeric) data into a numeric format so that machine learning algorithms can understand and use it.\n",
    "\n",
    "Most ML models, especially scikit-learn algorithms, work only with numbers, not text or labels.\n",
    "\n",
    "Encoding ensures categorical variables can be used as features in the model.\n",
    "\n",
    "Data Encoding Importants:\n",
    "\n",
    "ML models cannot process text or string data directly.\n",
    "\n",
    "Encoding preserves the information in categories in a numeric format.\n",
    "\n",
    "Helps algorithms like logistic regression, decision trees, SVM, neural networks to work with categorical data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a8866b-3a4b-4576-a6ea-24f77a87db8b",
   "metadata": {},
   "source": [
    "Common Data Encoding Techniques\n",
    "\n",
    "1. Label Encoding\n",
    "\n",
    "Converts each category into a unique integer.\n",
    "\n",
    "Example: Red → 0, Green → 1, Blue → 2\n",
    "\n",
    "Useful for ordinal categorical variables (with natural order)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1f5ad9b3-e9a6-412a-a892-ba82b2e414d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "colors = ['Red', 'Green', 'Blue', 'Green']\n",
    "le = LabelEncoder()\n",
    "encoded = le.fit_transform(colors)\n",
    "print(encoded)  # Output: [2 1 0 1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1328207a-e41a-4231-8d78-f9938a7fb91b",
   "metadata": {},
   "source": [
    "One-Hot Encoding\n",
    "\n",
    "Converts each category into a binary vector (0 or 1).\n",
    "\n",
    "Avoids implying any order between categories.\n",
    "\n",
    "Example: Red → [1,0,0], Green → [0,1,0], Blue → [0,0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0d5bac00-7b91-47c1-8a2b-235924d89e76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 2)\t1.0\n",
      "  (1, 1)\t1.0\n",
      "  (2, 0)\t1.0\n",
      "  (3, 1)\t1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "colors = np.array([['Red'], ['Green'], ['Blue'], ['Green']])\n",
    "ohe = OneHotEncoder()\n",
    "encoded = ohe.fit_transform(colors)\n",
    "print(encoded)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de59632b-9ff5-4090-ac1b-b0180945c689",
   "metadata": {},
   "source": [
    "Ordinal Encoding\n",
    "\n",
    "Assigns integers to categories based on a defined order.\n",
    "\n",
    "Example: Education Level → High School → 0, Bachelor → 1, Master → 2\n",
    "\n",
    "Binary Encoding / Hashing\n",
    "\n",
    "Converts categories into binary digits or hashes.\n",
    "\n",
    "Useful when there are many categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6417f365-ba50-437c-b7dc-d100227abd1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
